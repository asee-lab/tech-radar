"name","ring","quadrant","isNew","description"
ASP.NET Core,adopt,Frameworks and libraries,TRUE,"<p><strong><a href=""https://learn.microsoft.com/en-us/aspnet/overview"">ASP.NET Core</a></strong> is a modern, high-performance, open-source framework for building web applications and APIs. It is cross-platform, running on Windows, Linux, and macOS, and is designed for cloud and container-based deployments. It enables developers to build web apps, <a href=""https://learn.microsoft.com/en-us/aspnet/core/web-api/"">RESTful services</a>, and real-time applications using <strong>C#</strong> and <strong>.NET.</strong></p> <p>Key features include built-in dependency injection, a powerful middleware pipeline, unified <strong>MVC and Web API architecture</strong>, and support for <strong><a href=""https://learn.microsoft.com/en-us/aspnet/core/razor-pages/"">Razor Pages</a></strong> and <strong><a href=""https://learn.microsoft.com/en-us/aspnet/core/blazor/"">Blazor</a></strong>. <strong>ASP.NET Core</strong> is optimized for <a href=""https://learn.microsoft.com/en-us/aspnet/core/cloud/"">cloud</a> hosting and scalable deployments, and benefits from regular updates and strong community support.  <strong>ASP.NET Core</strong> is well adopted by teams within organization and developers can build robust, scalable, and maintainable applications using the latest technologies and best practices.</p>"
Entity Framework Core,adopt,Frameworks and libraries,TRUE,"<p><strong><a href=""https://learn.microsoft.com/en-us/ef/core/"">Entity Framework Core</a></strong> is a modern, open-source object-database mapper for .NET. It <strong>simplifies data access</strong> by allowing developers to work with databases using .NET objects, reducing the need for boilerplate SQL code and improving productivity. EF Core is <strong>highly practical for most application scenarios</strong>, offering strong integration with modern .NET, support for LINQ queries, migrations, and a <strong>wide range of relational databases</strong>.</p> <p>While EF Core is <strong>suitable for the majority of use cases</strong>, it is <strong>not a silver bullet</strong> — projects with extremely high performance or complex data access requirements may benefit from alternative solutions. For most applications, however, EF Core provides an efficient, maintainable, and developer-friendly approach to data access.</p>"
GSON,adopt,Frameworks and libraries,TRUE,"<p><a href=""https://github.com/google/gson"">Gson</a> is a mature and widely adopted Java library developed by Google for converting Java objects to JSON and vice versa. It offers simple and intuitive APIs, automatic mapping between Java objects and JSON, and robust handling of nulls, generics, and nested structures. Compared to Jackson, Gson is often preferred for projects that prioritize minimal configuration, ease of use, and a lightweight footprint, as it requires less boilerplate and is simpler to set up for small to medium-sized applications. While Jackson provides more advanced features and better performance for streaming large datasets or complex polymorphic mappings, Gson’s straightforward design and reliability make it an attractive choice for teams that value simplicity and maintainability. Its strong integration with popular Java frameworks, active community support, and proven stability make Gson a recommended technology for JSON serialization and deserialization in Java projects.</p>"
Hibernate,adopt,Frameworks and libraries,TRUE,"<p><a href=""https://hibernate.org/"">Hibernate</a> is a mature and widely adopted <strong>object-relational mapping</strong> (ORM) framework for Java, designed to simplify database interactions by mapping Java objects to relational database tables. It provides powerful features such as automatic SQL generation, caching, lazy loading, and transaction management, which help reduce boilerplate code and improve productivity. Hibernate also supports complex queries through HQL (Hibernate Query Language) and integrates seamlessly with popular Java frameworks like Spring. Its advantages include reducing development time, promoting maintainable and modular code, and supporting database portability. However, developers should be cautious, as improper configuration or misuse of features like lazy loading can lead to performance issues. Overall, Hibernate is a robust and proven technology, recommended for projects that require efficient and maintainable database interaction in Java applications.</p>"
Jackson,adopt,Frameworks and libraries,TRUE,"<p><a href=""https://github.com/FasterXML/jackson"">Jackson</a> is a widely used Java library for parsing, generating, and manipulating JSON data, providing developers with high-performance serialization and deserialization capabilities. Its support for streaming, tree-model processing, and flexible mapping between JSON and Java objects—including nested and polymorphic types—makes it suitable for a wide range of applications, from simple APIs to complex enterprise systems. Jackson integrates seamlessly with popular Java frameworks such as Spring Boot, and its extensive documentation, mature ecosystem, and active community ensure reliable support and rapid troubleshooting. Among its main advantages are speed, flexibility, and ease of integration, while potential challenges include the learning curve associated with advanced annotations, custom serializers, and deserializers. Overall, Jackson is a proven and dependable solution for JSON handling in Java projects, making it a strong candidate for adoption in modern applications.</p>"
JUnit,adopt,Frameworks and libraries,TRUE,"<p><a href=""https://junit.org/"">JUnit</a> is the de facto standard unit testing framework for Java applications, with JUnit 5 (Jupiter) representing a significant evolution from its predecessor. The framework provides a robust foundation for test-driven development with features like parameterized tests, dynamic tests, nested test classes, and improved extension mechanisms. JUnit 5&#39;s modular architecture separates the testing API from the execution engine, enabling better integration with build tools and IDEs. We consistently see JUnit as the backbone of Java testing strategies, with its mature ecosystem including extensive IDE support, seamless integration with build tools like Maven and Gradle, and compatibility with CI/CD pipelines. Teams adopting JUnit 5 benefit from modern Java features support, including lambdas and streams, while maintaining backward compatibility with <a href=""https://junit.org/junit4/"">JUnit 4</a> through the vintage engine. We strongly recommend JUnit for all Java projects. Its maturity, community support, and comprehensive feature set make it an essential tool in any Java developer&#39;s toolkit. Projects still using JUnit 4 should plan migration to JUnit 5 to leverage improved capabilities and ongoing support.</p>"
OpenTelemetry SDKs,adopt,Frameworks and libraries,TRUE,"<p>OpenTelemetry SDKs provide language-specific libraries for capturing traces, metrics and logs and exporting them to observability backends. Using the official OpenTelemetry SDKs ensures consistent telemetry signal structure across services, simplifies instrumentation, and enables vendor-neutral export through OTLP.</p> <p>Adopt OpenTelemetry SDKs for new services and when standardising observability across the platform. Prefer the official SDKs and stable exporters, follow semantic conventions, and centralise configuration (exporter endpoints, resource attributes) via environment variables or a shared configuration library.</p> <p>Notes: SDKs require careful sampling and resource tagging to avoid high cardinality and cost. When migrating, start with traces or metrics incrementally, validate dashboards and alerts, and ensure backwards compatibility for service-level SLIs.</p> <p>References: <a href=""https://opentelemetry.io/"">https://opentelemetry.io/</a></p>"
Spring Boot,adopt,Frameworks and libraries,TRUE,"<p><a href=""https://docs.spring.io/spring-boot/reference/index.html"">Spring Boot</a> enables rapid development while maintaining robustness and enterprise-grade capabilities.  Its ecosystem integration with Spring Cloud, Kubernetes, Docker, and observability tools makes it highly suitable for modern distributed architectures.  With strong community support and continuous evolution, Spring Boot remains one of the most stable and mature options in the Java ecosystem. While the framework simplifies setup, it can abstract away complexity, which may lead to reduced control over fine-grained configurations.  Memory usage can be higher compared to lighter frameworks, and teams should ensure proper understanding of auto-configuration behaviors to avoid hidden performance costs. Spring Boot is recommended for teams building scalable microservices or modular monolithic applications who want a mature ecosystem and strong support for DevOps practices. It should be considered a default choice for Java backend development unless specific performance constraints justify exploring lighter alternatives.</p>"
Angular Material,trial,Frameworks and libraries,TRUE,"<p><a href=""https://material.angular.dev/"">Angular Material</a> is a UI component library developed by the Angular team that implements Google’s Material Design principles for Angular applications. It provides a rich set of reusable, accessible, and responsive components such as buttons, dialogs, tables, form controls, and navigation elements. Built with Angular’s component-based architecture, Angular Material ensures seamless integration with Angular’s reactive forms, routing, and theming system. It supports customizable themes, dark mode, and internationalization, making it suitable for enterprise-grade applications. The library emphasizes accessibility (ARIA support) and performance, while offering utilities like CDK (Component Dev Kit) for building custom components. By standardizing on UI patterns from Google Material Design system, Angular Material accelerates development, improves consistency, and reduces design complexity in modern web applications.</p> <p>Angular Material is already in use across multiple projects in the group for internal agent-facing UI (where customer&#39;s do not expect their own design system) where it has proven valuable by providing ease of development and consistent styling across applications developed by disparate teams. Teams using Angular and building applications that do need to conform to specific design system are encouraged to try it.</p>"
Angular,trial,Frameworks and libraries,TRUE,"<p><a href=""https://angular.dev/"">Angular</a> is a comprehensive <a href=""https://www.typescriptlang.org/"">TypeScript</a>-based framework for building scalable single-page applications with opinionated patterns enforced through components, services, dependency injection, and a powerful CLI. The framework provides integrated solutions for routing, forms, HTTP clients, and testing with strong TypeScript and <a href=""https://rxjs.dev/"">RxJS</a> integration enabling sophisticated reactive data flows valuable in complex enterprise applications. <a href=""Angular Material"">Angular Material</a> offers production-ready UI components, while recent signals and improved change detection in Angular 16+ address performance concerns. The learning curve around RxJS and framework architecture is steeper than minimalist alternatives, and bundle sizes tend larger though lazy loading and build optimization mitigate production impact. Trial Angular for large-scale enterprise applications where teams value structure, comprehensive tooling, and consistent patterns across multiple teams requiring strong typing guarantees.</p>"
React JS,trial,Frameworks and libraries,TRUE,"<p>In the crowded landscape of front-end <a href=""https://ossinsight.io/collections/javascript-framework/"">JavaScript frameworks</a>, <a href=""https://react.dev/"">React.js</a> shows promise due to its reactive data flow, one-way data binding, and component-based architecture, which simplify rendering, promote code reuse, and make applications easier to maintain. Its virtual DOM enhances performance by minimizing unnecessary updates, and a rich ecosystem of libraries combined with strong community support accelerates development and troubleshooting. React.js is a mature and well-established framework, widely used across the globe—with surveys consistently ranking it among the top front-end frameworks—making it a low-risk choice to explore. We are currently trialing its use across projects of varying sizes while continuing to monitor alternatives like AngularJS, and based on initial observations, React.js could be a recommended technology for future front-end development.</p>"
Testcontainers,trial,Frameworks and libraries,TRUE,"<p><a href=""https://testcontainers.com/"">Testcontainers</a> is a library that provides lightweight, throwaway instances of databases, message brokers, web browsers, and other services running in Docker containers for integration testing. Originally developed for Java, Testcontainers now supports multiple languages including Python, Go, .NET, Node.js, and Rust, making it a polyglot solution for integration testing challenges. We&#39;re seeing strong adoption of Testcontainers as it addresses a longstanding pain point in integration testing — the gap between local development environments and production infrastructure. The library enables developers to write tests against real services rather than mocks or in-memory alternatives, significantly improving test reliability and confidence. Testcontainers supports a wide range of technologies through official modules (PostgreSQL, MySQL, Kafka, Redis, etc.) and generic container support for custom scenarios. Teams report that while initial setup requires Docker infrastructure, the investment pays off through reduced environment-related test failures and simplified CI/CD pipelines. We&#39;re placing Testcontainers in <strong>trial</strong> based on successful implementations across our projects. The main considerations are Docker runtime requirements and slightly longer test execution times compared to in-memory alternatives. However, the trade-off is worthwhile for projects requiring robust integration testing, particularly those working with complex data stores or distributed systems. Teams should ensure their CI environments support Docker and consider selective usage for critical integration paths.</p>"
Cypress,assess,Frameworks and libraries,TRUE,"<p><a href=""https://www.cypress.io/"">Cypress</a> is a JavaScript-based end-to-end testing framework that runs directly in the browser, offering a fundamentally different architecture from traditional Selenium-based tools. We&#39;re seeing increasing interest in Cypress due to its developer-friendly API, automatic waiting mechanisms, and superior debugging capabilities with time-travel snapshots. The framework excels at testing modern single-page applications and provides built-in support for API testing alongside UI tests. However, teams should be aware of its browser limitations—while it supports Chrome, Edge, and Firefox well, Safari support remains limited to paid tiers. We&#39;re placing Cypress in <strong>assess</strong> to encourage teams to evaluate it on greenfield projects where its benefits can be fully realized. Projects with existing Selenium investments should carefully consider the migration effort and team training requirements. Despite these considerations, Cypress&#39;s architecture addresses many pain points of traditional E2E testing, making it a compelling option for teams struggling with test reliability and maintenance overhead.</p>"
Quarkus,assess,Frameworks and libraries,TRUE,"<p>Quarkus is a Kubernetes-native Java framework that optimises for fast startup, low memory usage, and developer productivity (live reload, unified configuration). It supports GraalVM native images and integrates with Jakarta EE, MicroProfile and popular libraries.</p> <p>When to use</p> <ul> <li>New greenfield microservices, serverless functions or services where startup time and memory matter.</li> </ul> <p>Trade-offs</p> <ul> <li>Native-image builds increase CI complexity and time; some libraries need adaptation for native mode.</li> </ul> <p>Recommended action</p> <ul> <li>Adopt for new cloud-native Java services; assess migrations carefully and plan native-image builds and testing early. Develop and test with a Kubernetes environment in mind (see <a href=""https://www.jetbrains.com/help/idea/quarkus.html"">IntelliJ Quarkus support</a> and <a href=""Kubernetes"">Kubernetes</a>).</li> </ul>"
ApexCharts,hold,Frameworks and libraries,TRUE,"<p><a href=""https://apexcharts.com/"">ApexCharts</a> is a popular JavaScript charting library known for its simplicity, modern aesthetics, and rich set of features for building interactive visualizations. It has been widely adopted in web applications for rendering dynamic charts with minimal configuration and solid documentation.</p> <p>However, as of <strong>June 2025</strong>, ApexCharts has changed its <strong>licensing terms</strong> to require a paid license for <strong>commercial or business use</strong>. This shift has significant implications for enterprise projects, especially in environments that prioritize open-source or cost-neutral components.</p> <p>We place ApexCharts in the <strong>Hold</strong> ring due to this licensing change. While technically still a capable library, the updated licensing model introduces <strong>compliance, cost, and procurement overhead</strong> that makes it less attractive for continued or new usage in business contexts. Teams should <strong>reconsider its use for new projects</strong> and evaluate alternatives with more permissive licenses such as <a href=""https://www.chartjs.org/"">Chart.js</a>, <a href=""https://echarts.apache.org/en/index.html"">ECharts</a>, or <a href=""https://github.com/plotly/plotly.py"">Plotly</a> (open-core).</p>"
ASP.NET,hold,Frameworks and libraries,TRUE,"<p><strong><a href=""https://learn.microsoft.com/en-us/aspnet/overview"">ASP.NET</a></strong> is an <strong>older</strong> web framework that is <strong>no longer actively developed and receives only minimal maintenance updates</strong>. Developers should hold off on adopting classic ASP.NET for new projects, as it lacks many modern features, performance improvements, and cross-platform capabilities found in <strong><a href=""ASP.NET Core"">ASP.NET Core</a></strong>. For future-proof, scalable, and maintainable applications, it is recommended to use ASP.NET Core instead.</p>"
AutoMapper,hold,Frameworks and libraries,TRUE,"<p><strong><a href=""https://automapper.org/"">AutoMapper</a></strong> is a popular library that simplifies object-to-object mapping, reducing boilerplate code when transferring data between models and DTOs. However, as of 2024, AutoMapper has <strong>moved to a commercial license for new versions</strong>, requiring payment for continued use in many scenarios. Developers should hold off on adopting AutoMapper for new projects unless they are comfortable with the licensing costs.</p> <p>For most use cases, <strong>alternatives like <a href=""https://github.com/MapsterMapper/Mapster"">Mapster</a></strong> offer similar functionality with a more permissive license and active development. Consider evaluating these alternatives before committing to AutoMapper.</p>"
Bootstrap,hold,Frameworks and libraries,TRUE,"<p>Bootstrap is a popular front-end framework based on HTML, CSS, and JavaScript that facilitates the rapid development of consistent and adaptive web interfaces. Its grid system and predefined components streamline prototyping and the creation of responsive layouts.</p> <p>Although it was a key tool for many years, its adoption has <a href=""https://2023.stateofcss.com/en-US/css-frameworks/#bootstrap"">declined</a> with the rise of modern frameworks such as Tailwind CSS, Material UI, and native React or Vue components. Bootstrap often results in visually uniform and less customizable interfaces, while adding unnecessary weight to the front end. Maintaining Bootstrap implies accepting a rigid visual coupling and a high technical cost in future refactoring efforts. In addition, frequent security vulnerabilities demand ongoing maintenance to apply updates.</p> <p>It is recommended to keep Bootstrap only in legacy projects and plan its eventual migration, while avoiding its use in new initiatives in favor of more modular and maintainable alternatives.</p>"
.NET Framework,hold,Frameworks and libraries,TRUE,"<p><strong><a href=""https://dotnet.microsoft.com/en-us/download/dotnet-framework"">.NET Framework</a></strong> is a <strong>legacy platform that is no longer actively developed, receiving only critical maintenance and security updates</strong>. Developers should hold off on adopting the old .NET Framework for new projects, as it lacks the performance, cross-platform support, and <strong>modern features available in the latest <a href=""https://dotnet.microsoft.com/en-us/"">.NET</a></strong> versions. For new development, it is recommended to use modern .NET, which offers better scalability, maintainability, and future-proofing.</p>"
jQuery,hold,Frameworks and libraries,TRUE,"<p><a href=""https://jquery.com/"">jQuery</a> is a JavaScript library that simplifies DOM manipulation, event handling, and AJAX requests, providing cross-browser compatibility and concise syntax for interacting with HTML elements. It was a cornerstone of front-end web development for more than a decade, enabling developers to build interactive and dynamic interfaces with far less code compared to vanilla JavaScript.</p> <p>However, its usage has drastically declined with the <a href=""https://2023.stateofjs.com/en-US/other-tools/#libraries"">rise of modern frameworks</a> and libraries such as React, Vue, and Svelte, as well as the improvement of native browser APIs that make DOM manipulation more efficient and standardized. In new projects, jQuery introduces unnecessary complexity and potential performance overhead. It is advisable to retain it only in legacy systems and plan its gradual removal to improve maintainability, scalability, and performance consistency.</p>"
MediatR,hold,Frameworks and libraries,TRUE,"<p>MediatR is a popular open-source .NET library that implements the <a href=""https://en.wikipedia.org/wiki/Mediator_pattern"">Mediator pattern</a> for in-process messaging, often used to help enforce Command Query Responsibility Segregation (<a href=""CQRS"">CQRS</a>) and vertical slice architecture. It decouples the request sender (e.g., a controller) from the request handler (the business logic) by routing commands and queries to their respective single handlers, and publishing notifications to multiple handlers. This separation aims to reduce controller &quot;constructor explosion&quot; and enforce the Single Responsibility Principle.</p> <p>MediatR has recently transitioned from a fully permissive open-source license (MIT) to a dual-license model (Reciprocal Public License 1.5/Commercial).</p>"
Newtonsoft.Json,hold,Frameworks and libraries,TRUE,"<p><a href=""https://www.newtonsoft.com/json"">Newtonsoft.Json</a> has long been the dominant JSON library for .NET, offering rich features like LINQ to JSON (JObject/JArray), custom converters, and extensive contract customization. It remains a mature, high-quality library with broad community adoption and a proven track record across enterprise systems.</p> <p>However, it’s now placed in the <strong>Hold</strong> ring due to strategic and ecosystem reasons rather than technical shortcomings. The .NET platform has fully matured its native <a href=""https://learn.microsoft.com/en-us/dotnet/api/system.text.json"">System.Text.Json</a> library, which provides high-performance, memory-efficient serialization and is now the framework default. Maintaining dual serialization stacks adds unnecessary complexity, increases maintenance effort, and complicates cross-library type handling.</p> <p>Teams should avoid new dependencies on Newtonsoft.Json and migrate gradually to System.Text.Json unless a legacy contract or polymorphic converter still mandates it. Hold does not imply removal but signals that continued new adoption no longer aligns with platform direction or maintainability goals.</p>"
Clean Architecture,adopt,Patterns and Practices,TRUE,"<p><a href=""https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html"">Clean Architecture</a> is a software design approach that emphasizes separation of concerns, independence from frameworks, and testability by organizing code into concentric layers with strict dependency rules. The core principle is that dependencies point inward toward business logic, ensuring the domain layer remains independent of external concerns like UI, databases, or frameworks. Various implementations exist including <strong>layered architecture</strong>, <strong>hexagonal architecture</strong>, and <strong>ports and adapters</strong> patterns, all sharing the goal of decoupling business rules from infrastructure details.</p> <p>Clean Architecture provides significant benefits including improved testability through isolated business logic, flexibility to swap frameworks or databases without impacting core functionality, and clearer code organization that aids long-term maintainability. Teams adopting these patterns consistently report better separation between domain logic and technical implementation, making systems easier to understand, modify, and extend. We&#39;re placing Clean Architecture in <strong>adopt</strong> based on its proven effectiveness across projects of varying sizes and complexity. Teams should apply these principles pragmatically, understanding that the investment in proper layering and dependency management pays dividends through reduced coupling and enhanced maintainability. For new projects and major refactoring efforts, particularly with frameworks like <a href=""Spring Boot"">Spring Boot</a>, Clean Architecture provides a solid foundation for building sustainable, evolvable systems.</p>"
Continuous delivery,adopt,Patterns and Practices,TRUE,"<p><a href=""https://docs.aws.amazon.com/es_es/wellarchitected/latest/devops-guidance/continuous-delivery.html"">Continuous Delivery (CD)</a> automates the preparation and deployment of software, enabling each version to be potentially releasable at any time. This practice relies on automated pipelines that validate, package, and deploy changes consistently. It takes after <a href=""Continuous integration"">Continuous integration</a>. </p> <p>Its implementation ensures a traceable, automated software lifecycle aligned with principles of observability and immutable deployments. It also increases delivery speed, operational stability, and business responsiveness. By reducing manual errors and facilitating a predictable deployment flow, Continuous Delivery (CD) becomes a key pillar for multi-organization ecosystems or environments with multiple production stages. It further enhances collaboration between development and operations [DevOps] (../PP/devops.md) teams.</p>"
Continuous integration,adopt,Patterns and Practices,TRUE,"<p><a href=""https://www.atlassian.com/continuous-delivery/continuous-integration"">Continuous Integration (CI)</a> is a <a href=""DevOps"">DevOps</a> practice that automates code compilation, testing, and validation every time a developer makes changes to the repository. It ensures that the software remains always integrable and functional.</p> <p>Adopting <a href=""https://docs.aws.amazon.com/es_es/wellarchitected/latest/devops-guidance/continuous-integration.html"">Continuous Integration (CI)</a> allows teams to detect issues early, improve code quality, and reduce integration time across teams. Continuous Integration promotes discipline in collaborative development and sets the foundation for more advanced practices such as <a href=""Continuous delivery"">Continuous delivery</a> and test automation. In complex or distributed enterprise environments, Continuous Integration (CI) adoption is a need and results in greater traceability, source code consistency, and version control.</p>"
DevOps,adopt,Patterns and Practices,TRUE,"<p>What</p> <ul> <li>DevOps is the combination of cultural practices, automation, and measurable operational processes (CI/CD, Infrastructure as Code, automated testing, observability, and continuous delivery) that enable fast, safe, repeatable delivery of software.</li> </ul> <p>Why now</p> <ul> <li>Business demands faster delivery and reliable operations. DevOps reduces lead time, increases deployment frequency and improves recovery time with small, testable changes and continuous feedback loops.</li> </ul> <p>When to adopt</p> <ul> <li>Adopt when you need faster iteration, better alignment between teams, predictable releases, and improved reliability. Start with CI, trunk-based development, automated tests, and progressive delivery; grow toward IaC, observability and SRE practices.</li> </ul> <p>Considerations</p> <ul> <li>Culture change is the hardest part: invest in cross-functional teams, shared SLIs/SLOs, and blameless postmortems.</li> <li>Prioritize automation and incremental rollout; avoid big-bang transformations.</li> <li>Secure by design: integrate security checks early (shift-left).</li> </ul> <p>Key practices</p> <ul> <li>Continuous Integration / Continuous Delivery (CI/CD)</li> <li>Infrastructure as Code (Terraform, CloudFormation)</li> <li>Observability (metrics, logs, traces)</li> <li>Trunk-based development and feature flags</li> <li>Blameless postmortems and SLO-driven operations</li> </ul> <p>Further reading</p> <ul> <li><a href=""https://about.google/intl/en/products/sre/"">https://about.google/intl/en/products/sre/</a></li> <li><a href=""https://martinfowler.com/bliki/ContinuousDelivery.html"">https://martinfowler.com/bliki/ContinuousDelivery.html</a></li> <li><a href=""https://12factor.net"">https://12factor.net</a></li> </ul>"
Git branching strategies,adopt,Patterns and Practices,TRUE,"<p>The chosen <a href=""https://docs.aws.amazon.com/prescriptive-guidance/latest/choosing-git-branch-approach/git-branching-strategies.html"">branching strategies</a> directly impacts integration frequency, release management, and the overall developer experience. While branching provides isolation and safety, too much structure can slow teams down, while too little governance can lead to instability. Two commonly adopted strategies illustrate this balance:</p> <p>GitFlow represents a more structured approach with dedicated branches for features, releases, hotfixes, and long-term maintenance. It introduces a clear separation between ongoing development and production-ready code, making it suitable for teams that manage multiple release versions or operate under strict release governance. However, this structure comes with operational overhead and may slow down integration cycles if not automated and disciplined.</p> <p>Trunk-Based Development (TBD) takes a contrasting approach by encouraging developers to integrate changes directly into the main branch or through very short-lived feature branches. This strategy emphasizes continuous integration, frequent merging, and rapid feedback loops, making it a strong fit for CI/CD-driven teams aiming for fast and incremental delivery. While it reduces complexity and merge conflicts, it relies heavily on strong automation and testing practices to maintain stability.</p>"
Infrastructure as Code,adopt,Patterns and Practices,TRUE,"<p><a href=""https://en.wikipedia.org/wiki/Infrastructure_as_code"">Infrastructure as Code</a> expresses compute, network, storage, and platform configuration as versioned, reviewable code. Declarative definitions produce reproducible environments, reduce configuration drift, and make changes auditable. Provisioning tools such as Terraform manage cloud and on-prem resources, while Ansible and image builders like Packer configure operating systems and middleware. Kubernetes manifests are packaged with Helm or shaped with Kustomize and reconciled by GitOps controllers such as Argo CD or Flux.  Guardrails arise naturally from the workflow: pull requests, automated plans, policy checks, and continuous reconciliation highlight unintended changes before they reach production. Secrets remain outside version control in a vault or KMS, and remote state backends provide locking and encryption. The result is faster recovery through re-creation, consistent environments across data centers, and a clear change history that satisfies operational and compliance needs.</p>"
Microfrontends,adopt,Patterns and Practices,TRUE,"<p>Microfrontends extend <a href=""Microservices"">Microservices</a> principles to frontend development by decomposing user interfaces into independently deployable, team-owned modules that compose at runtime or build time. Each microfrontend encapsulates a business domain with its own technology stack, deployment pipeline, and lifecycle, integrating through framework-agnostic patterns like <a href=""https://www.webcomponents.org/"">web components</a>, module federation, iframe isolation, or server-side composition. Common integration approaches include <a href=""https://webpack.js.org/concepts/module-federation/"">Webpack Module Federation</a> for runtime sharing, <a href=""https://single-spa.js.org/"">single-spa</a> for framework orchestration, or edge-side composition with CDN routing. The architecture enables autonomous team velocity with independent deployments, technology diversity for evolving frameworks, and incremental migration from monolithic frontends. Challenges include managing shared dependencies to prevent bundle duplication, coordinating cross-cutting concerns like authentication and styling, maintaining consistent UX across boundaries, and ensuring performance with multiple framework runtimes. Adopt microfrontends for large-scale applications with multiple teams requiring independent delivery cadences and clear domain boundaries, establishing governance for shared libraries, design systems, and communication protocols between modules.</p>"
OpenID Connect,adopt,Patterns and Practices,TRUE,"<p><a href=""https://openid.net/developers/how-connect-works/"">OpenID Connect</a> (OIDC) is an identity layer built on top of the OAuth 2.0 protocol that enables secure user authentication and single sign-on (SSO) across applications. It allows clients to verify the identity of an end user based on authentication performed by an authorization server, and to obtain basic profile information in an interoperable and REST-like manner. OIDC introduces the concept of an ID Token, a JSON Web Token (JWT) that contains claims about the user and the authentication event. It supports flows like Authorization Code, Implicit, and Hybrid, making it suitable for both server-side and client-side applications. By leveraging standard endpoints (e.g., /authorize, /token, /userinfo), OIDC simplifies integration with identity providers such as Google, Azure AD, and Okta.</p> <p>It is widely adopted for modern architectures, including microservices and APIs, because it provides a secure, standardized way to handle authentication without managing credentials directly.</p>"
Structured Logging,adopt,Patterns and Practices,TRUE,"<p><strong>Structured logging</strong> provides significant benefits by capturing log data in a consistent, machine-readable format, such as JSON. This approach enables powerful analytics and querying capabilities, making it easier to identify patterns, troubleshoot issues, and monitor application health. Structured logs are especially practical during load tests and high-traffic scenarios, as they allow for efficient aggregation and analysis of large volumes of data. By adopting structured logging, teams can improve observability, streamline debugging, and gain deeper insights into system behavior.</p>"
Agile Development,trial,Patterns and Practices,TRUE,"<p>Agile development is an iterative and people-centric approach to software delivery, governed by the [<strong>Agile Manifesto</strong>&#39;s] (<a href=""https://agilemanifesto.org/"">https://agilemanifesto.org/</a>) four values and twelve principles. It prioritizes <strong>Individuals and interactions</strong> over processes and tools, <strong>Working software</strong> over comprehensive documentation, <strong>Customer collaboration</strong> over contract negotiation, and <strong>Responding to change</strong> over following a plan. It breaks work into short, fixed-length cycles (<strong>Sprints</strong> or iterations), with frequent feedback loops and continuous delivery of working software, allowing teams to adapt quickly to evolving requirements and customer needs. Agile benefits are proven, its successful adoption is highly dependent on organizational and team maturity, but it introduces new risks.</p> <p>A new project should selectively pilot Agile practices (like daily stand-ups, short iterations, and continuous integration) to test the team&#39;s ability to live the Agile mindset. Simply &quot;doing&quot; the rituals without the underlying culture of collaboration, self-organization, and unwavering customer focus (the &quot;being&quot; Agile part) can lead to <strong>scope creep</strong>, <strong>technical debt</strong>, and a <strong>lack of predictability</strong>. The project must first validate that the organization is ready to provide active customer involvement and empowered, cross-functional teams before scaling up.</p>"
API First,trial,Patterns and Practices,TRUE,"<p><a href=""https://www.postman.com/api-first"">API First</a> is an approach to software design where APIs are treated as first-class citizens and defined before any implementation begins. Instead of building an application and then exposing APIs as an afterthought, teams start by designing the API contract—often using specifications like OpenAPI or AsyncAPI—to ensure clarity, consistency, and interoperability.</p> <p>This approach is currently used succesfully by many teams in our group, and we encourage others to try it in their future projects. Designing APIs first promotes <strong>consumer-driven design</strong>, enabling <strong>parallel development across teams</strong>, better <strong>alignment with business requirements</strong>, and seamless <strong>integration within distributed or microservices architectures</strong>. It also supports automated workflows such as <strong>mock servers</strong>, <a href=""Contract testing"">Contract testing</a>, and <strong>code generation</strong>, helping reduce errors and accelerate delivery. By prioritizing APIs early, organizations create more <strong>maintainable</strong>, <strong>scalable</strong>, and <strong>reusable services</strong>, which is critical for modern ecosystems where APIs are the backbone of digital platforms.</p>"
CQRS,trial,Patterns and Practices,TRUE,"<p><a href=""https://martinfowler.com/bliki/CQRS.html"">CQRS (Command Query Responsibility Segregation)</a> separates read and write operations into distinct models where commands modify state through domain logic enforcing business rules while queries retrieve data optimized for specific use cases without side effects. This enables independent scaling and evolution of read/write paths, particularly valuable in high-throughput systems with asymmetric patterns. CQRS pairs naturally with <a href=""https://martinfowler.com/eaaDev/EventSourcing.html"">event sourcing</a> for audit trails and temporal queries, allowing varied projections of the same data—customer views, fulfillment operations, analytics, search—each optimized for access patterns and consistency requirements. The pattern introduces complexity through synchronization mechanisms, eventual consistency reasoning, projection rebuilds, and schema evolution across models, often via <a href=""Event-driven architecture"">Event-driven architecture</a> or message queues. Trial CQRS in bounded contexts with high read/write asymmetry, specialized query needs, complex domain logic, or audit requirements, avoiding simple CRUD operations or scenarios requiring strict immediate consistency.</p>"
Microservices,trial,Patterns and Practices,TRUE,"<p><a href=""https://microservices.io/"">Microservices</a> is an architectural style that organizes software into small, independently deployable services with clear bounded contexts. Each service owns its data and exposes contracts through APIs or events, enabling teams to iterate and scale components separately. Communication favors asynchronous messaging for decoupling and resilience, with synchronous calls reserved for request/response needs and protected by timeouts, retries, and circuit breakers. Operational autonomy improves when services publish uniform health endpoints, metrics, and traces, allowing platform tooling to automate rollouts and failure handling. The approach introduces new responsibilities—contract versioning, schema evolution, cross-service debugging, and eventual consistency—mitigated by patterns such as outbox/CDC, sagas for multi-step flows, and schema registries. Microservices fit best where domain boundaries are strong and deployment velocity matters more than in-process calls.</p>"
AI for articulating requirements,assess,Patterns and Practices,TRUE,"<p><strong>AI-powered tools</strong> have the potential to transform high-level ideas into actionable requirements, intelligently generating specifications, acceptance criteria, and edge cases while improving alignment between stakeholders and engineering teams. They can accelerate requirement drafting, highlight gaps or ambiguities early, and enhance communication between technical and non-technical participants. However, these tools are still maturing: <a href=""https://www.thoughtworks.com/insights/blog/generative-ai/using-ai-requirements-analysis-case-study"">AI-generated requirements</a> may lack context-specific nuance, over-reliance can lead to incomplete or biased specifications, and integration with existing requirement management systems may require additional setup. As such, AI-driven requirement articulation is currently best positioned for assessment and experimentation, allowing teams to explore its benefits while carefully validating outputs and monitoring reliability.</p>"
AI for code generation,assess,Patterns and Practices,TRUE,"<p>AI-powered code generation tools (GPT-based assistants, Codex, and similar) can accelerate developer workflows by scaffolding boilerplate, suggesting idiomatic patterns, and producing small, focused code snippets. They are useful for boosting productivity, onboarding, and generating test or example code quickly.</p> <p>Why now</p> <ul> <li>Large language models have reached practical utility for many coding tasks and integrate with editors and CI pipelines.</li> </ul> <p>When to use</p> <ul> <li>Use for boilerplate generation, exploratory prototyping, and as an assistive tool during reviews and learning. Treat generated code as a starting point and review for correctness, security, and style.</li> </ul> <p>Caveats / Considerations</p> <ul> <li>Generated code may contain subtle bugs, insecure patterns, or licensing concerns depending on the training data. Always review, test, and run static analysis. Maintainable adoption requires guardrails (unit tests, linters, code reviews) and team guidance on scope of use.</li> </ul> <p>References</p> <ul> <li>Example: editor integrations and CLI tools (GitHub Copilot, open-source alternatives).</li> </ul>"
AI for code review,assess,Patterns and Practices,TRUE,"<p>AI for code review uses large language models to read diffs, files, and project context, then surface potential bugs, security smells, style inconsistencies, and missing tests. It typically runs as an IDE assistant or a PR bot that leaves inline comments, suggests patches, summarizes changes, and explains risky areas—speeding up reviews.</p> <p>It shines in day-to-day development and CI/CD: catching copy-paste mistakes and off-by-ones, highlighting unsafe API usage, proposing docstrings and unit tests, flagging secret leaks, and summarizing large pull requests. For small-to-medium diffs and repetitive patterns across services, AI can reduce reviewer toil and improve consistency. Teams can standardize it via CI jobs that label issues by severity and auto-apply safe refactors behind the scenes.</p> <p>Performance advice may lack profiling evidence; and security findings are not a substitute for <a href=""https://owasp.org/www-community/Source_Code_Analysis_Tools"">SAST</a>/<a href=""https://owasp.org/www-community/Attacks/Attack_Surface_Analysis"">DAST</a> and dependency audits. Privacy, IP protection, and compliance matter: sending proprietary code to third-party services may be unacceptable without enterprise controls, and costs can grow with large repos and frequent runs.</p> <p>Use AI code review to augment your pipeline, not gate it alone. Treat AI comments as non-blocking suggestions; require human approval for merges. Pair it with your existing quality stack—formatters/linters, unit/integration tests, SAST (e.g., Semgrep/<a href=""https://docs.github.com/code-security/code-scanning/introduction-to-code-scanning/about-code-scanning-with-codeql"">CodeQL</a>), SCA, and secret scanning.</p>"
AI for codebase Q&A,assess,Patterns and Practices,TRUE,"<p>Engineering teams in the industry are exploring AI assistants that answer natural-language questions about code, aiding developer onboarding and support.  These tools combine large language models with semantic code search across large, multi-language codebases. <a href=""https://docs.github.com/en/enterprise-cloud@latest/copilot/concepts/context/repository-indexing"">GitHub Copilot Semantic Search</a> integrates into IDEs to provide code explanations and assist with refactoring. Similarly, Sourcegraph’s <a href=""https://sourcegraph.com/cody"">Cody</a> and enterprise assistants like <a href=""https://www.glean.com"">Glean</a> index entire repositories and use embeddings to answer questions with code context, enabling queries like “Where is this function used?”   However, accuracy remains a concern, as models can hallucinate nonexistent code or answers. Solutions mitigate this via retrieval-grounded answers and fine-grained access controls (like self-hosting and repository permissions). Latency and indexing overhead are trade-offs, but the productivity benefits make this a rising trend.</p>"
AI for documentation authoring,assess,Patterns and Practices,TRUE,"<p>AI for Documentation Authoring refers to the use of Large Language Models (LLMs) and specialized tools to <strong>automatically generate, augment, and manage technical documentation</strong>. This includes generating first drafts of user manuals, creating comprehensive API documentation directly from code annotations, summarizing lengthy design documents, and performing real-time gap analysis between code and existing documentation. The primary goal is to <strong>reduce manual effort</strong>, ensure documentation stays <strong>up-to-date</strong> with code changes, and improve overall <strong>consistency and clarity</strong>. </p> <p>Technique has high promise for efficiency, balanced by critical risks, especially in new projects, and you should explore using AI tools for repetitive or structural documentation tasks like generating basic code comments or setting up document templates. The gains in velocity can be significant. Make sure to use AI responsibly and in accordance to company policy.</p>"
AI for rapid prototyping,assess,Patterns and Practices,TRUE,"<p>Generic AI tools such as <a href=""https://chat.openai.com"">ChatGPT</a>, <a href=""https://claude.ai"">Claude</a> and more specialized ones such as <a href=""https://v0.dev/"">v0</a> are rapidly transforming how product teams move from idea to prototype. Generative AI models can translate natural-language narratives into low-fidelity UI wireframes, demo code, or product concept decks within minutes or hours. Beyond simple code or text generation, they support interactive iterations, suggesting UI variations, creating user flows, and even generating back-end stubs for simulated demos. </p> <p>For teams in our group, this capability shortens the <strong>idea → prototype → feedback</strong> cycle, allowing PMs and designers to test more ideas per iteration without requiring full engineering effort. It also empowers cross-functional discovery sessions, where early user flows or mock apps can be generated in minutes and reviewed collaboratively with stakeholders. By automating the tedious setup work, these AI tools free teams to focus on user value and design intent. While AI accelerates experimentation, outputs must still be validated for UX quality, compliance, security, and technical feasibility. Teams should assess this practice as a scalable enabler of rapid innovation and a way to compress product discovery timelines without overloading engineering capacity or design teams.</p>"
AI for test from requirements,assess,Patterns and Practices,TRUE,"<p>AI-powered tools for generating test cases directly from requirements represent an emerging approach to address the time-consuming process of test planning and scenario creation. We&#39;re observing early experimentation with large language models (LLMs) and specialized AI tools that analyze requirement documents, user stories, or acceptance criteria to automatically generate test scenarios, test cases, and even executable test scripts. These tools promise to accelerate test coverage analysis, identify edge cases that humans might overlook, and reduce the initial effort in test design. Early adopters report value in using AI as a starting point for test case generation, particularly for exploratory testing ideas and coverage gap identification. However, the outputs require significant human review and refinement—AI-generated tests often lack business context, may miss critical domain-specific scenarios, and can produce brittle or maintenance-heavy test scripts. We&#39;re placing this practice in <strong>assess</strong> as the technology shows potential but remains immature. Teams interested in exploring this space should treat AI as an augmentation tool rather than a replacement for skilled testers. Start with small experiments on non-critical features, establish clear review processes, and measure the actual time savings against the refinement effort. The real value may lie in sparking ideas and improving test coverage rather than fully automated test generation.</p>"
AI for ui mockups,assess,Patterns and Practices,TRUE,"<p>AI-driven UI generation from high-level prompts is gaining traction in late 2025. Tools like <a href=""https://stitch.design.google"">Google’s Stitch</a> (formerly Galileo) and Uizard’s <a href=""https://uizard.io/autodesigner"">Autodesigner</a> produce multi-screen layouts from a brief description. Vercel’s <a href=""https://v0.dev"">v0</a> can even output working React code from natural-language specs. <a href=""https://www.figma.com"">Figma</a>  now offer AI “first draft” features, though these lack design-system integration and polish.</p> <p>Open-source experiments (e.g., W&amp;B’s <a href=""https://wandb.ai/openui"">OpenUI</a>) let teams explore prompt-to-UI generation with custom LLMs. AI coding assistants like Locofy use design-trained models to turn mockups into front-end code. Some research prototypes integrate vision-capable LLMs (for sketch recognition) and layout optimization, but results still need human refinement. Digital Platform is exploring ways to generate declarative UI conforming to ASEE Design System from natural language prompts.</p> <p>In summary, generative UI techniques show promise for rapid prototyping and variant exploration, helping designers and PMs quickly visualize concepts, but these tools aren’t yet reliable for production-ready UI design.</p>"
AI for Unit Tests Generation,assess,Patterns and Practices,TRUE,"<p>Using <strong>AI for unit test generation</strong> can significantly accelerate the process of creating tests, improve coverage, and help identify edge cases that might otherwise be missed. AI tools can quickly generate a large number of test cases based on existing code, saving developers time and effort.</p> <p>However, it is essential that <strong>developers remain in control</strong> of the test case spectrum. Developers should carefully review, curate, and supplement AI-generated tests to ensure they are meaningful, relevant, and aligned with business requirements. <strong>Relying solely on AI can lead to gaps in coverage or tests that do not reflect real-world scenarios.</strong></p> <p><strong>To maximize the effectiveness</strong> of AI-driven test generation from technical aspect, it is important to <strong>provide a solid foundation</strong> by manually writing initial test cases. This setup seed establishes the patterns and expectations for the codebase, giving the AI clear guidance and context to generate more accurate and useful tests. By <strong>combining developer expertise with AI capabilities</strong>, teams can achieve higher quality and more maintainable test suites.</p>"
Contract testing,assess,Patterns and Practices,TRUE,"<p>Traditional integration tests spin up full environments and exercise multiple services simultaneously, which introduces heavy setup costs, long execution times, and high flakiness where a small environment or data issue can break dozens of tests.</p> <p>In contrast, <strong>contract tests</strong> focus on the API agreements between services: consumers and/or providers define expectations, and each provider verifies that it still satisfies those expectations. Because these tests run in isolation with lightweight mocks or stubs, they detect compatibility issues early in CI pipelines without requiring full system orchestration.</p> <p><strong>Consumer-driven contracts</strong> and tools such as <a href=""https://pact.io/"">Pact</a> let downstream consumers define assertions that are executed in provider flows. In addition, <strong>provider-driven</strong> contract flows validate backward compatibility with API conformance tests and contract change validation. Modern setups integrate contract tests into CI to flag breaking API changes early and publish verified contracts to registries like <a href=""https://github.com/pact-foundation/pact_broker"">Pact Broker</a>.</p> <p>Teams should treat contract tests as executable documentation between squads and as input to release automation. The challenge is governance: versioning, schema evolution, and when to allow breaking changes.</p>"
Domain Driven Design,assess,Patterns and Practices,TRUE,"<p>Domain-Driven Design (DDD) is an architectural philosophy and set of practices centered around connecting the <strong>implementation</strong> with an evolving model of the <strong>core business domain</strong>. It involves deep collaboration with domain experts to distill a <strong>Ubiquitous Language</strong>—a shared vocabulary used by both developers and business users in the code and documentation. Key patterns include <strong>Aggregates</strong>, <strong>Entities</strong>, and <strong>Value Objects</strong>, all operating within explicitly defined <strong>Bounded Contexts</strong> to manage complexity and ensure the software accurately reflects the business&#39;s reality.</p> <p>DDD has potential for <strong>high returns on complex problems</strong>, balanced by a <strong>significant initial investment</strong>. You should explore DDD principles when the domain complexity is a primary concern. It excels at breaking down large problems and aligning software with business strategy. However, DDD requires deep domain expertise, developer discipline, and introduces patterns that can be over-engineering for simple CRUD applications. Teams should first <strong>research and investigate</strong> the approach to understand its cognitive load and ensure the business problem is complex enough to warrant the cost before committing to full adoption.</p>"
Event-driven architecture,assess,Patterns and Practices,TRUE,"<p><a href=""https://en.wikipedia.org/wiki/Event-driven_architecture"">Event-driven architecture</a> is a software design pattern where components react to and communicate through the production and detection of events, which are changes in system state. Instead of waiting for a request, services are triggered by events and can respond to them asynchronously, leading to a more loosely coupled, scalable, and resilient system</p> <p>Producers and consumers don&#39;t need to know about each other, only the event format. This makes it easier to update or replace components independently. The system can scale easily because individual services can be scaled up or down based on their event load without affecting others. If a consumer service goes down, the event broker can hold the events until it comes back online, making the system more fault-tolerant. It allows applications to react to information in real time, which is crucial for use cases like fraud detection, inventory management, and personalized user experiences.</p>"
Factory acceptance testing,assess,Patterns and Practices,TRUE,"<p>Factory Acceptance Testing (FAT), is a pre-shipment process where deliverables are tested by vendor to verify it meets all specifications, design criteria, and functional requirements before being released to the market. When a product team delivers a packaged software for on-premises deployment, <strong>factory acceptance testing (FAT)</strong> ensures that product deliverables operate correctly in a realistic, production-like environment. FAT typically encompasses multiple testing dimensions — functional verification, performance benchmarking, and basic security validation — depending on product type and risk profile. The practice increasingly relies on automation frameworks to execute regression and performance suites and can vary in scope from module-level validation of subsystems to full product-level end‑to‑end acceptance.</p> <p>Adoption will help teams reduce defect escape rate, post-delivery incidents, ensure environment reproducibility, and shorten customer acceptance cycles. The main challenge remains in balancing realistic test environments with efficient automation.</p>"
OSS license checks,assess,Patterns and Practices,TRUE,"<p>With frequent changes of licenses in open-source projects and LLM tools producing snippets they learned from open codebases, open-source license compliance is becoming a necessary part of continuous delivery pipelines. Modern tools like <a href=""https://snyk.io/"">Snyk</a>, <a href=""https://fossa.com/"">FOSSA</a>, <a href=""https://github.com/nexB/scancode-toolkit"">ScanCode</a> can detect non-compliant or risky licenses (GPL, AGPL, SSPL) before release. Integrating automated license verification into CI/CD protects us from downstream legal exposure and helps procurement vet third-party dependencies at scale.</p> <p>Teams should explorie policy-as-code enforcement to block merges or releases when high-risk licenses appear. The goal is to move license validation from manual review to a repeatable, auditable pipeline control, embedded in the same way we treat static analysis or vulnerability scans today. Over time this practice should become mature part of our secure software supply-chain posture — but adoption cost and scanning noise remain real challenges.</p>"
Spec-driven development,assess,Patterns and Practices,TRUE,"<p>Spec-driven development (AI-automated) treats a <strong>living spec</strong> (plain-language goals, constraints, acceptance criteria) as the source of truth for agents that <strong>plan → scaffold → implement → validate</strong> via PRs and checks. In practice, teams operate along three maturity levels: <strong>spec-first</strong> (spec guides one change), <strong>spec-anchored</strong> (spec persists and evolves), and <strong>spec-as-source</strong> (humans mostly edit specs; code syncs to them). Tooling spans IDE/CLI workflows like <a href=""https://kiro.dev/"">Kiro</a> (requirements → design → tasks), GitHub’s <a href=""https://github.com/github/spec-kit/"">Spec Kit</a> (constitution + specify/plan/tasks loop), and <a href=""https://tessl.io/"">Tessl</a> (spec-anchored, exploring spec-as-source). Similar patterns appear in frameworks such as <a href=""https://github.com/bmad-code-org/BMAD-METHOD"">BMAD</a> and <a href=""https://github.com/Fission-AI/OpenSpec"">OpenSpec</a>. <strong>Assess</strong> for teams piloting agentic workflows, platform engineering, and greenfield features; keep human approval gates and encode non-negotiables in the spec.</p>"
Cloud Native Storage,adopt,Runtime Infrastructure,TRUE,"<p>Cloud-native storage solutions have become critical components for running stateful applications in Kubernetes environments, providing persistent, scalable, and resilient storage that integrates seamlessly with container orchestration. We&#39;re seeing strong adoption of solutions like <a href=""https://rook.io/"">Rook-Ceph</a>, which automates Ceph storage cluster deployment and management in Kubernetes, providing block, file, and S3-compatible object storage capabilities, and <a href=""https://longhorn.io/"">Longhorn</a>, a lightweight distributed block storage system originally developed by Rancher. Other notable open-source alternatives include <a href=""https://openebs.io/"">OpenEBS</a>, which offers container-attached storage with multiple storage engines, and <a href=""https://min.io/"">MinIO</a> for object storage compatible with S3 APIs. These solutions provide flexibility and avoid vendor lock-in while delivering enterprise-grade features like replication, snapshots, and backup capabilities. For organizations preferring commercial support, solutions like <a href=""https://portworx.com/"">Portworx</a> offer advanced features including disaster recovery and data security, while <a href=""https://www.purestorage.com/"">Pure Storage Portworx</a> provides enterprise-grade storage with comprehensive support and SLAs. We&#39;re placing cloud-native storage in <strong>adopt</strong> based on its maturity and necessity for production Kubernetes deployments. The key considerations are choosing the right solution based on workload requirements—block storage for databases, file storage for shared data, or object storage for unstructured data. Teams should evaluate performance requirements, operational complexity, and the trade-offs between self-managed open-source solutions and commercial offerings. Start with pilot projects to understand operational characteristics and ensure your team has the expertise to manage distributed storage systems. For organizations running stateful applications in Kubernetes, implementing a robust cloud-native storage solution is no longer optional but a fundamental requirement for production readiness.</p>"
ContainerD,adopt,Runtime Infrastructure,TRUE,"<p>Containerd is an industry-standard container runtime that serves as the foundation for containerized applications across modern infrastructure ecosystems. It provides a reliable, secure, and efficient way to manage container lifecycles, from image management to process execution. </p> <p>Kubernetes transitioned from Docker as the default container runtime to containerd on May 3, 2022 Redhat Openshift switched from Docker runtime to CRI-O in summer 2019. For kubernetes based workloads CRI-O has better performance and has low resource usage.</p> <p>For Desktops Containers Windows : Use Rancher Desktop for GUI-driven workflows or Podman for daemonless operations Macos   : Rancher Destktop provides a GUI and switchable runtime, uses nerdctl (for containerd) or Docker CLI (for dockerd/Moby) Linux   : Podman is a good alternative. Docker desktop is resource intensive, requires licensing for commercial use</p> <pre><code>Distro               Runtime                              RedHat OpenShift     CRI-O                               Rancher              CRI-O/containerd                    openSUSE Kubic       CRI-O                               kubernetes vanilla   usually containerd, both possible   MicroK8s             containerd                          Docker Enterprise    Moby                                </code></pre>"
Gitlab CI/CD,adopt,Runtime Infrastructure,TRUE,"<p>GitLab CI/CD is a powerful, built-in <strong>Continuous Integration and Continuous Delivery</strong> system that is a core part of the GitLab DevOps platform. It automates the entire software delivery process—from building and testing to deploying and monitoring—using a simple, version-controlled YAML configuration file (<code>.gitlab-ci.yml</code>) stored alongside your code. It utilizes <strong>Runners</strong> to execute jobs, supporting complex workflows like parallel execution, caching, and integrated security scanning (DevSecOps), all within the same environment as your Git repository.</p> <p>GitLab CI/CD represents a mature, low-friction standard for setting up modern development pipelines. For new projects, adopting a mature CI/CD system from day one is mandatory for maintaining a healthy codebase and high development velocity. The key advantage of GitLab CI/CD is its <strong>single-platform integration</strong>: you don&#39;t need to install, integrate, or maintain separate CI servers and source control. This significantly <strong>reduces setup overhead</strong> and the likelihood of toolchain issues. The YAML syntax is widely understood, and the platform provides excellent <strong>templates and auto-DevOps</strong> features, making it the default choice to ensure a reliable, automated pipeline is active from the very first commit.</p>"
Kubernetes with enterprise support,adopt,Runtime Infrastructure,TRUE,"<p>As more of our clients adopt cloud‑native applications that depend on Kubernetes for deployment, the need for consistent and secure infrastructure support has grown. For regulated clients who demand on‑premises or hybrid deployments, we have arranged <strong>enterprise-supported Kubernetes distributions</strong> such as <a href=""https://ubuntu.com/kubernetes"">Canonical  Kubernetes</a> with Canonical MSP (managed service provider) agreement ASEE RS  and <a href=""https://www.redhat.com/en/technologies/cloud-computing/openshift"">Red Hat OpenShift</a> with  agreement Payten Turkey signed with IBM. These offerings provide L2-L3 support, hardened images, and certified patching that ensure security and lifecycle management under vendor accountability. Enterprise support simplifies compliance audits, vulnerability management (CVE tracking), and provides predictable upgrade paths. Teams serving regulated clients should adopt one of the distributions we already work with.</p> <p>In contrast, <strong>vanilla Kubernetes</strong> distributions with community support deliver flexibility and rapid updates but come with higher operational risk. Maintaining control planes, patching nodes, and handling API deprecations require deep internal expertise. Without vendor SLAs, outages, regressions and patching depend solely on our internal  capacity. While community-supported environments may work fine for dev/test workloads on R&amp;D clusters, our internal capacity to provide guarantees necessary for production workloads should be carefully assessed.</p>"
Kafka,adopt,Runtime Infrastructure,TRUE,"<p><a href=""https://kafka.apache.org/"">Apache Kafka</a> provides a durable, high-throughput event log that decouples producers from consumers. Topics are partitioned for scale and replicated for fault tolerance, allowing independent services to publish and subscribe without direct knowledge of each other.Backpressure is handled naturally by consumer offsets, and replay is as simple as re-reading from an earlier offset. Schema enforcement with Avro or Protobuf protects compatibility, while compaction and retention policies align storage with business semantics. Common patterns include event-driven integration, change-data-capture pipelines, streaming ETL, and audit trails. Healthy clusters maintain in-sync replicas, even partition distribution, and predictable latency; observability centers on controller state, request timing, consumer lag, and under-replicated partitions. Exactly-once processing exists for narrow paths, yet most designs rely on idempotent producers and consumers for simplicity and resilience.</p>"
Keycloak,adopt,Runtime Infrastructure,TRUE,"<p><a href=""https://www.keycloak.org/"">Keycloak</a> is an open-source identity and access management solution that provides comprehensive authentication and authorization capabilities for modern applications. We&#39;ve seen Keycloak mature into a robust platform that supports industry-standard protocols including OpenID Connect, OAuth 2.0, and SAML 2.0, making it suitable for both new applications and legacy system integration. The platform offers features like single sign-on (SSO), social login integration, user federation with LDAP/Active Directory, multi-factor authentication, and fine-grained authorization policies. Teams appreciate Keycloak&#39;s admin console for managing users, roles, and clients, along with its extensive customization options through themes and service provider interfaces. We&#39;re placing Keycloak in <strong>adopt</strong> based on its proven track record across our projects. It significantly reduces the complexity and development effort required to implement secure authentication and authorization, while providing enterprise-grade features out of the box. The main considerations are proper infrastructure sizing for high-availability deployments and understanding its operational requirements. Organizations should invest in learning Keycloak&#39;s realm and client configuration patterns, as proper setup is crucial for security and maintainability. For teams building applications that require authentication, Keycloak eliminates the need to build these capabilities from scratch and provides a battle-tested foundation for identity management.</p>"
Kubernetes,adopt,Runtime Infrastructure,TRUE,"<p><a href=""https://kubernetes.io/"">Kubernetes</a> is an orchestration platform that schedules and manages containerized workloads through declarative APIs. Deployments, StatefulSets, and Jobs describe desired state; the control plane reconciles actual state continuously. Service discovery, rolling updates, horizontal and vertical autoscaling, and self-healing provide robust day-to-day operations. A rich extension model—CRDs, operators, CSI for storage, CNI for networking, and Ingress/Gateway for traffic—supports diverse workloads from stateless web services to databases. Security and multi-tenancy benefit from namespaces, RBAC, network policies, and admission control. Effective clusters pair resource requests/limits with pod disruption budgets and readiness probes to enable safe rollouts. Observability integrates metrics, logs, and traces, commonly via Prometheus, centralized logging, and OpenTelemetry. Kubernetes standardizes deployment and runtime behavior across environments, enabling consistent delivery practices at scale.</p>"
Low CVE base images,adopt,Runtime Infrastructure,TRUE,"<p>Adopting low CVE base images like Alpine Linux in your container deployments is a strategic shift toward improved security. This approach significantly reduces the attack surface and eliminates unnecessary components in containerized production environments, also minimizes the memory and disk requirements.</p> <p>Implementation Strategy<br>    Verify and audit current base images. Identify where container images are used.      Using the base image add necessary components or choose an application specific image like java/nginx.     Ensure all necessary dependencies are available in base package repositories</p> <p>Application Notes     Base images are minimal and do not contain the basic tools. If you are debugging the container, you may not find your network tool.</p> <p>What options are available for low-CVE base images? </p> <pre><code>Base Image                Size (MB)   Known CVEs (approx.)            notes                                                            Alpine                    5-7             0-5 (minimal due to few packages)   very mature                                                     Alpaquita Linux           3-4             0-5 (Alpine + patching)             java optimized with BellSoft, free images, support commercial   Debian Slim               70-80           50-100 (mostly low-severity)        larger image                                                    Chainguard Wolfi (base)   10-15           0 (zero-CVE policy )                commercial general images                                       BusyBox                   1-2             0-2 (ultra-minimal)                 no package manager, limited                                     </code></pre>"
NGINX,adopt,Runtime Infrastructure,TRUE,"<p>NGINX is a high-performance HTTP server and reverse proxy widely used for load balancing, TLS termination, static content delivery, and as an edge gateway. It scales well with modest resource use, has a small operational footprint, and integrates smoothly with container orchestration and CI/CD pipelines.</p> <p>When to use</p> <ul> <li>Front your services with NGINX for reliable TLS termination and layer 7 routing.</li> <li>Use as a performant static file server or as an ingress controller in Kubernetes.</li> <li>Prefer NGINX when predictable high concurrency and low-latency routing are required.</li> </ul> <p>Advice and trade-offs</p> <ul> <li>Adopt default NGINX for most edge and proxy needs; use NGINX Plus only when enterprise features/support are required.</li> <li>Invest in secure default configs (rate limiting, header sanitization, TLS hardening) and automated config testing.</li> <li>For advanced API gateway features consider combining with dedicated gateway solutions; for simple use-cases NGINX is often sufficient.</li> </ul> <p>Resources</p> <ul> <li><a href=""https://nginx.org"">https://nginx.org</a></li> <li>Kubernetes ingress controllers: NGINX Ingress Controller / ingress-nginx</li> <li>Migration patterns: reverse-proxy-first, gradual cutover, health-check-driven failover</li> </ul>"
PostgreSQL,adopt,Runtime Infrastructure,TRUE,"<p><a href=""https://www.postgresql.org/"">PostgreSQL</a> is an advanced open-source relational database system providing ACID guarantees, sophisticated query optimization, and extensive data type support including JSON, arrays, hstore, and custom types. The database offers powerful indexing strategies (B-tree, GiST, GIN, BRIN, hash), full-text search, window functions, CTEs, and advanced SQL features for complex analytical queries. PostgreSQL&#39;s extensibility through custom functions, operators, procedural languages (PL/pgSQL, PL/Python, PL/Java), and extensions like <a href=""https://postgis.net/"">PostGIS</a> for geospatial data, <a href=""https://www.timescale.com/"">TimescaleDB</a> for time-series, and <a href=""https://github.com/pgpartman/pg_partman"">pg_partman</a> for partitioning enables specialized workloads. Robust concurrency control through MVCC, streaming replication, logical replication, and point-in-time recovery provide high availability and disaster recovery capabilities. Performance tuning involves query planning analysis with EXPLAIN, appropriate indexing strategies, connection pooling via <a href=""https://www.pgbouncer.org/"">PgBouncer</a>, and configuration optimization for shared buffers, work memory, and WAL settings. Adopt PostgreSQL as the default relational database for applications requiring data integrity, complex queries, extensibility, and proven operational maturity across diverse workloads from transactional systems to analytical platforms.</p>"
Microsoft SQL Server,adopt,Runtime Infrastructure,TRUE,"<p><strong><a href=""https://learn.microsoft.com/en-us/sql/sql-server/"">Microsoft SQL Server</a></strong> is a robust, enterprise-grade relational database platform that offers extensive features for both development and production environments. Developers should adopt SQL Server because it provides seamless out-of-the-box integration with .NET applications, simplifying setup and data access during development. Its tooling, management studio, and support for local development make it easy to get started and maintain productivity.</p> <p>For production, SQL Server delivers high reliability, scalability, advanced security features, and rich support for analytics, reporting, and high availability. It is well-suited for mission-critical applications, offering comprehensive backup, monitoring, and performance tuning capabilities. With strong community and enterprise support, SQL Server remains a top choice for building and running modern data-driven applications.</p>"
ASEE Flow,trial,Runtime Infrastructure,TRUE,"<p><a href=""https://contentservices.asee.io/asee-flow/"">ASEE Flow</a> is a <strong>drop-in replacement for Camunda 7 Community Edition</strong>, developed by <strong>ASEE</strong> to ensure continuity after Camunda 7 CE end-of-life in October 2025. It preserves <strong>full API, BPMN, and runtime compatibility</strong> with <strong>Camunda 7.23/7.24</strong>, enabling seamless migration without code changes. ASEE Flow replaces the legacy Camunda Cockpit and Tasklist with a <strong>modern React-based interface</strong> providing improved process visibility and simplified troubleshooting.</p> <p>Already integrated into the <a href=""https://docs.asee.io/platform/platform-microservices/bpm/"">ASEE Digital Platform BPM Microservice</a>, ASEE Flow is in <strong>trial</strong> as we evaluate operational characteristics across projects. Teams using Camunda 7 CE directly without <strong>Digital Platform</strong> should plan migration before CE support ends, testing in non-production environments first. For projects with significant investment in Camunda 7, ASEE Flow offers a low-risk path forward with maintained compatibility and ASEE-backed support.</p>"
Elasticsearch,trial,Runtime Infrastructure,TRUE,"<p><a href=""https://www.elastic.co/elasticsearch"">Elasticsearch</a> is a distributed search and analytics engine built on Lucene. It shines for full-text search, log analytics, and fast aggregations over large, time-ordered datasets. Data is indexed into shards and replicated across nodes for high availability and near-real-time queries.</p> <p>It’s a strong fit for centralized application logs, application search, and exploring operational metrics. Before wider adoption, a trial should validate: index and shard sizing, hot-warm-cold storage tiers, Index Lifecycle Management (ILM) for retention, and snapshot/restore to S3/MinIO for disaster recovery. Keep in mind JVM/heap tuning, shard counts, and ingestion pressure—these can impact stability and cost. Elasticsearch is not a primary system of record; model for eventual consistency and design updates/deletes carefully.</p>"
RabbitMQ,trial,Runtime Infrastructure,TRUE,"<p><a href=""https://www.rabbitmq.com/docs"">RabbitMQ</a> is an open-source messaging system based on the AMQP protocol that enables asynchronous communication between distributed components. It is widely used to decouple services and manage message queues reliably.</p> <p>RabbitMQ is a mature, stable technology with a strong ecosystem, making it ideal for microservices architectures or systems requiring resilience and concurrent processing. Its implementation can enhance scalability and fault tolerance. It is particularly suitable for work queue patterns due to its robust messaging and persistence capabilities.</p> <p>For architectures that need to handle continuous, large-scale event streams—such as event streaming or data analytics systems, <a href=""Kafka"">Kafka</a> may be a more appropriate <a href=""https://aws.amazon.com/compare/the-difference-between-rabbitmq-and-kafka/?nc1=h_ls"">choice</a>. A validation phase against alternatives such as Kafka or Azure Service Bus is recommended, assessing message nature, persistence requirements, and expected throughput.</p>"
Redis,assess,Runtime Infrastructure,TRUE,"<p><a href=""https://redis.io/"">Redis</a> is our low-latency accelerator for caching, counters, lightweight queues, sessions, and rate limits—not a source of truth. We run it on Kubernetes with an operator and treat placement as a resilience tool: leaders are spread, followers are kept on different nodes, and pods are balanced to avoid single-node risk. The main lessons are conceptual: design for bounded memory so structures never grow without limits; choose a sensible eviction strategy and avoid wide, blocking scans. Optimize client behavior by pooling connections, batching with pipelining, and reducing hot-key contention. Keep production safe with strict command hygiene and access control, preferring iterative scans over risky commands. Be intentional about persistence, selecting cache-only or append-only/snapshot modes based on recovery needs rather than habit. Build in security and isolation from the start, and observe everything—latency, throughput, memory health, replication—so drift triggers timely alerts. Finally, codify these practices in scheduling rules, resource sizing, and simple restore drills to keep Redis fast, predictable, and boring.</p>"
S3 Compatible Object Storage,assess,Runtime Infrastructure,TRUE,"<p>Object storage is a type of data storage architecture that manages data as objects, as opposed to other storage architectures like file systems which manage data as a file hierarchy, or block storage which manages data as blocks within sectors and tracks. Each object typically includes the data itself, metadata, and a unique identifier.</p> <p><a href=""https://aws.amazon.com/s3/"">S3</a> represents a widely adopted standard for object storage, originally developed by Amazon Web Services (AWS). The S3 API allows users to store and retrieve any amount of data at any time from anywhere on the web. It is designed for scalability, durability, and low-latency access. The API of S3 has become a de facto standard for object storage, and many other cloud providers and storage solutions have implemented S3-compatible APIs to ensure interoperability.</p> <p>S3 can be supported by <a href=""Cloud Native Storage"">Cloud Native Storage</a>, which are designed to integrate seamlessly with cloud-native applications and environments.</p>"
ActiveMQ Artemis,hold,Runtime Infrastructure,TRUE,"<p><a href=""https://activemq.apache.org/components/artemis/"">ActiveMQ Artemis</a> is a performant, feature-rich message broker implementing JMS 2.0 with support for AMQP, MQTT, STOMP, and OpenWire. It offers high throughput, low latency, and flexible configuration for enterprise messaging workloads.</p> <p>We place Artemis in the <strong>Hold</strong> ring primarily due to persistent <strong>operational challenges in Kubernetes and cloud-native environments</strong>. Artemis’s architecture depends on shared state and journal-based persistence, which conflicts with stateless, horizontally scalable design principles. Setting up <strong>high availability (HA)</strong> requires complex coordination using shared file systems or replicated databases, which are brittle under container orchestration. Moreover, Artemis lacks native autoscaling, operator maturity, and observability integrations compared to modern distributed logs or streaming platforms.</p> <p>For new workloads, teams should prefer brokers designed from the ground up for partitioned, state-distributed operation with stronger Kubernetes support and operational simplicity such as <a href=""https://kafka.apache.org"">Kafka</a> or <a href=""https://www.redpanda.com"">RedPanda</a>. Artemis remains viable for legacy JMS integration but unsuitable as a cloud-native backbone.</p>"
Docker Compose,hold,Runtime Infrastructure,TRUE,"<p>Docker Compose is a tool for defining and running <a href=""https://docs.docker.com/compose/"">multi-container Docker applications</a>. Using a YAML file (<code>docker-compose.yml</code>), developers can configure services, networks, and volumes, then start everything with a single command. This simplifies development, testing, and CI/CD workflows.  </p> <p>Docker Compose excels in <strong>local development</strong> and <strong>small-scale deployments</strong>, offering an easy way to define dependencies, environment variables, ports, and volumes. It enables teams to quickly replicate production-like environments or run integration tests across multiple containers.  </p> <p>However, Docker Compose is <strong>not suitable for production orchestration</strong>. It lacks native capabilities for scaling, self-healing, rolling updates, distributed service discovery, and advanced load balancing. Operating primarily on a single host, it does not provide clustering, high availability, or enterprise-grade observability.  </p> <p>Organizations should use Docker Compose for <strong>development and testing only</strong>, while production workloads should run on orchestration platforms like <a href=""https://kubernetes.io/"">Kubernetes</a>, <a href=""https://docs.docker.com/engine/swarm/"">Docker Swarm</a>, or managed container services such as <a href=""https://aws.amazon.com/ecs/"">Amazon ECS</a> or <a href=""https://cloud.google.com/kubernetes-engine"">Google Kubernetes Engine (GKE)</a>. Teams currently using Compose in production should plan migration to a more robust solution, while continuing to leverage Compose for its intended purpose — <strong>simplifying the developer experience in non-production environments</strong>.</p>"
Graylog,hold,Runtime Infrastructure,TRUE,"<p><a href=""https://graylog.org/"">Graylog</a> is a centralized log management platform that provides log collection, indexing, search, and analysis capabilities. While Graylog excels at log aggregation with features like powerful search queries, dashboards, and alerting, it is <strong>not part of a comprehensive observability stack</strong> that integrates logging, tracing, distributed tracing, events, and metrics into a unified platform.</p> <p>Modern observability requires correlating logs with traces and metrics to understand system behavior holistically. Graylog focuses primarily on log management and lacks native integration with tracing frameworks and metrics collection, requiring additional tools to build a complete observability solution. This fragmented approach increases operational complexity and makes it harder to troubleshoot distributed systems effectively. We&#39;re placing Graylog in <strong>hold</strong>, recommending teams adopt integrated observability stacks like <a href=""Grafana observability stack for IM and APM"">Grafana observability stack for IM and APM</a> (Loki, Tempo, Mimir), Elastic Observability (Elasticsearch, APM), or cloud-native solutions that provide unified logging, tracing, and metrics. For new monitoring projects, teams should prioritize platforms that offer comprehensive observability rather than standalone logging solutions.</p>"
IdentityServer,hold,Runtime Infrastructure,TRUE,"<p>IdentityServer is an authentication and authorization framework based on the OpenID Connect and OAuth 2.0 standards, designed to manage tokens, roles, and access control in distributed applications across diverse environments and architectures.</p> <p>While it is a solid tool, its <a href=""https://docs-duendesoftware-com.translate.goog/general/licensing/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sge#:~:text=IdentityServer%20is%20free%20for%20development,production%20use%20requires%20a%20license"">licensing</a> and support model have changed significantly from October 2020, limiting the viability of free usage. IdentityServer4 ended support in December 2022. This change can lead to the tool being left unmaintained, which carries security, compliance, and long-term compatibility risks. It is recommended that if maintaining it up to date is not feasible, teams should evaluate alternative solutions and plan migration paths proactively.</p>"
Internet Information Services,hold,Runtime Infrastructure,TRUE,"<p><a href=""https://www.iis.net/"">Internet Information Services</a> (IIS) is Microsoft’s long-standing web server for hosting web applications and services on Windows. It integrates tightly with the Windows ecosystem and supports ASP.NET, .NET Core, PHP, and other frameworks, providing features such as application pools, SSL/TLS, URL rewriting, and diagnostics tools. Historically, IIS served as the backbone of enterprise .NET hosting and intranet deployments.</p> <p>However, we have placed IIS in the <strong>Hold</strong> ring due to its <strong>inherent incompatibility with cloud-native and containerized architectures</strong>. Its dependency on the Windows platform makes it difficult to deploy, scale, and maintain within Kubernetes environments. Configurations rely on stateful system management rather than declarative infrastructure, limiting automation and portability. Additionally, it carries higher resource overhead and Windows licensing costs.</p> <p>Modern workloads should standardize on <strong>Kestrel</strong> (for .NET) or <strong>NGINX</strong> as lightweight, cross-platform web servers that align with containerized, cloud-native practices and deliver simpler scaling and operational models.</p>"
Oracle Database,hold,Runtime Infrastructure,TRUE,"<p><a href=""https://www.oracle.com"">Oracle Database</a> remains one of the most capable enterprise RDBMS platforms, offering advanced HA, clustering, partitioning, and security features. Technically, it delivers strong performance, rich SQL and PL/SQL capabilities, and proven reliability for mission-critical workloads. It offers advanced features such as ACID compliance, multitenant architecture (CDB/PDB), Real Application Clusters (RAC) for high availability, and Data Guard for disaster recovery.</p> <p>We have placed Oracle Database in the <strong>Hold</strong> ring due to its <strong>high total cost of ownership</strong> (TCO) and <strong>limited compatibility with modern development ecosystems</strong>. Integration with modern ecosystems (Kubernetes, Kafka, observability, CI/CD pipelines) is cumbersome, while vendor lock-in limits cloud portability. Several ASEE projects have faced challenging contract negotiations and restrictive licensing that negatively impacted our timelines and revenue.</p> <p>We recommend <strong>PostgreSQL</strong> and <strong>Microsoft SQL Server</strong> as strategic alternatives that balance functionality, ecosystem integration, and supportability. Existing Oracle workloads should be <strong>maintained</strong> but no new systems should adopt it unless <strong>mandated by client constraints</strong>.</p>"
VMware vSphere,hold,Runtime Infrastructure,TRUE,"<p><a href=""https://www.vmware.com/products/cloud-infrastructure/vsphere"">VMware vSphere</a> is an enterprise virtualization platform providing comprehensive capabilities for running virtual machines and managing data center infrastructure. While vSphere has been a dominant solution with proven stability, <strong>Broadcom&#39;s acquisition of VMware</strong> has disrupted the market with dramatic <strong>price increases</strong> and <strong>licensing model changes</strong>, forcing organizations to reconsider their virtualization strategies.</p> <p>Broadcom shifted to subscription-only models with substantial price hikes, bundled previously separate products, and eliminated lower-tier editions. This has accelerated interest in alternatives like <strong>Proxmox</strong>, <strong>OpenStack</strong>, and <strong>Kubernetes</strong>. <strong>ASEE is focusing new projects on Kubernetes</strong> for cloud-native infrastructure, aligning with industry trends toward container orchestration. We&#39;re placing VMware vSphere in <strong>hold</strong>, recommending teams pause new vSphere investments. Existing deployments should continue operating while assessing migration paths. For new infrastructure projects, teams should prioritize Kubernetes-based solutions with <a href=""Cloud Native Storage"">Cloud Native Storage</a> unless specific vSphere dependencies justify the increased investment.</p>"
Containers and K8S on dev machine,adopt,Tools,TRUE,"<p><strong>Containers and Kubernetes</strong> on development machines represent a mature and proven approach to cloud-native development, enabling teams to build and test software in production-like environments locally. This practice bridges the gap between development and production by allowing developers to run containerized applications, debug Kubernetes manifests, and validate infrastructure-as-code changes in isolated setups.</p> <p>The ecosystem has evolved beyond traditional Docker Desktop. <a href=""https://rancherdesktop.io/"">Rancher Desktop</a> provides a stable, lightweight Kubernetes distribution with built-in container runtime support, while <a href=""https://podman.io/"">Podman</a> offers a daemonless, Docker-compatible engine that integrates cleanly with <a href=""https://kind.sigs.k8s.io/"">kind</a> and <a href=""https://minikube.sigs.k8s.io/"">minikube</a> for local cluster management. <a href=""Docker Compose"">Docker Compose</a> remains valuable for simpler use cases, but local Kubernetes delivers closer alignment with production orchestration models.</p> <p>Teams adopting this approach benefit from improved environment consistency and faster feedback cycles. However, running full Kubernetes locally can be resource-intensive and may differ from managed cloud clusters in networking and storage behavior. Emerging options such as <strong>Dev Containers</strong> and <strong>GitHub Codespaces</strong> are extending these concepts to cloud-hosted development environments.  </p> <p>Learn more in the <a href=""https://kubernetes.io/docs/home/"">official Kubernetes documentation</a>.</p>"
Figma,adopt,Tools,TRUE,"<p><a href=""https://www.figma.com/prototyping/"">Figma</a> stands out as a cloud-based design tool that empowers real-time collaboration, streamlines design-to-development handoffs, and keeps teams aligned with a single, centralized workspace. Unlike traditional desktop-based design tools such as Adobe XD or Sketch, Figma allows multiple users to work simultaneously on the same file without version conflicts, providing a seamless collaborative experience. Teams can create and iterate on <strong>UI/UX designs</strong>, <strong>wireframes</strong>, and <strong>prototypes</strong> while maintaining centralized design systems for consistency. By reducing feedback loops and streamlining the handoff between design and development, Figma accelerates workflows for both co-located and distributed teams. While it relies on internet connectivity and requires careful version and access management in large organizations, it is recommended for teams seeking a cloud-first, collaborative design solution that integrates smoothly with agile development practices.</p>"
Git,adopt,Tools,TRUE,"<p><a href=""https://git-scm.com/"">Git</a> is the distributed version control system and de facto standard for source code management, providing every developer a complete repository history for offline work, fast branching, merging, and cryptographic integrity guarantees. The distributed architecture supports workflows from <a href=""https://trunkbaseddevelopment.com/"">trunk-based development</a> to complex <a href=""https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow"">gitflow</a> strategies, with universal ecosystem integration across IDEs, CI/CD platforms, <a href=""GitHub"">GitHub</a>, <a href=""GitLab"">GitLab</a>, and Bitbucket for pull requests, code review, and automation. Git handles repositories from small projects to massive monorepos efficiently, though advanced features like rebasing, cherry-picking, and reflog require operational expertise. Teams must establish branching strategy conventions, commit message formats, and repository hygiene practices for binary files, <a href=""https://git-lfs.com/"">Git LFS</a>, and sensitive data. Adopt Git as the standard version control system—its ubiquity ensures developer familiarity, universal tooling support, and mature solutions for all version control needs.</p>"
GitHub,adopt,Tools,TRUE,"<p><a href=""https://github.com/"">GitHub</a> is the leading platform for <a href=""Git"">Git</a> repository hosting and code collaboration, providing pull requests for code review, issue tracking, project management, CI/CD automation via <a href=""https://github.com/features/actions"">GitHub Actions</a>, package management, and security scanning with <a href=""https://github.com/dependabot"">Dependabot</a> and <a href=""https://codeql.github.com/"">CodeQL</a>. The platform&#39;s pull request workflow with draft PRs, suggested changes, automated checks, required reviews, and branch protection rules has become the industry standard for maintaining code quality. GitHub Actions offers powerful CI/CD with marketplace integrations, while security features include secret scanning and dependency vulnerability alerts. Integration with IDEs and development tools is ubiquitous, and the platform hosts the world&#39;s largest open-source community. <a href=""https://github.com/enterprise"">GitHub Enterprise</a> provides SSO, audit logs, advanced security policies, and self-hosted deployment options for compliance requirements. Adopt GitHub as the primary platform for source code management and collaboration across all project sizes.</p>"
GitLab,adopt,Tools,TRUE,"<p><a href=""https://about.gitlab.com/"">GitLab</a> is a comprehensive DevOps platform that provides source code management, CI/CD pipelines, container registry, security scanning, and project management capabilities in a single application. We&#39;ve consistently observed GitLab&#39;s evolution from a GitHub alternative into a complete DevOps lifecycle tool that reduces toolchain complexity and integration overhead. The platform&#39;s built-in CI/CD with GitLab Runners, merge request workflows, and Auto DevOps features enable teams to implement modern software delivery practices without stitching together multiple tools. GitLab&#39;s security features including SAST, DAST, dependency scanning, and container scanning are particularly valuable for organizations with compliance requirements. Both self-hosted and SaaS options provide flexibility for different organizational needs, with the self-hosted option offering greater control over data and infrastructure. We&#39;re placing GitLab in <strong>adopt</strong> based on widespread successful implementations across our projects. It delivers significant value through reduced tool sprawl, unified visibility across the development lifecycle, and strong CI/CD capabilities. Teams should be prepared for GitLab&#39;s comprehensive feature set, which can be overwhelming initially — start with core Git workflows and gradually adopt additional features. Organizations already invested in other DevOps toolchains should evaluate migration costs carefully, but for new projects or teams looking to consolidate their tooling, GitLab provides an excellent all-in-one solution.</p>"
Harbor,adopt,Tools,TRUE,"<p>Harbor is a production-ready, open-source container registry that adds RBAC, image signing, vulnerability scanning, replication and Helm/OCI support on top of a Docker distribution. We recommend adopting Harbor for organisation-wide registries and CI/CD workflows that need centralized scanning and policy enforcement; note it requires backing services (database, Redis, object storage) and operational care for backups, TLS, and upgrades.</p> <p>References: <a href=""https://goharbor.io/"">https://goharbor.io/</a> — <a href=""https://goharbor.io/docs/"">https://goharbor.io/docs/</a></p>"
Helm,adopt,Tools,TRUE,"<p><a href=""https://helm.sh"">Helm</a> is the de facto package manager for <a href=""Kubernetes"">Kubernetes</a>. It packages Kubernetes resources as charts, supports versioned releases, lifecycle upgrades/rollbacks, and chart discovery via repositories/OCI registries. Helm is tool for everyday <a href=""DevOps"">DevOps</a></p> <p>Why now</p> <ul> <li>Mature ecosystem, wide adoption, strong tooling (linting, testing, artifact registries). Charts speed reproducible installs and simplify complex templates for teams deploying many services.</li> </ul> <p>When to use</p> <ul> <li>Use Helm to standardize and templatize Kubernetes deployments, manage upgrades and rollbacks, and distribute curated, versioned operational patterns across teams.</li> </ul> <p>Caveats / Considerations</p> <ul> <li>Templating complexity can grow; prefer small focused charts and clear values.yaml.</li> <li>Pay attention to chart provenance, signed charts or OCI registry controls, and secret handling (avoid embedding secrets in values).</li> <li>For GitOps workflows consider combining Helm with a GitOps controller (Argo CD, Flux) or evaluate Kustomize if you prefer pure overlays.</li> </ul>"
Jira,adopt,Tools,TRUE,"<p>Jira is a premier software development management platform from Atlassian. Originally a bug tracker, it has evolved into a comprehensive tool for <strong>Agile project management</strong> that supports Scrum, Kanban, and custom methodologies. Jira&#39;s core function is to organize, track, and manage <strong>work items (Issues)</strong>—including user stories, tasks, and bugs—through customizable workflows. It offers features like backlogs, sprint boards, roadmaps, reporting (e.g., burndown charts), and extensive APIs/integrations, making it a central hub for development, testing, and even non-technical teams.</p> <p>Jira is a excellent pick when starting new projects, as establishing clear work tracking is foundational to <a href=""Agile Development"">Agile Development</a> delivery. Adopting Jira early provides immediate process standardization and visibility. It&#39;s the industry standard for professional software teams, meaning it minimizes the learning curve for new hires and integrates seamlessly with nearly every other popular development tool (GitHub, GitLab, <a href=""Gitlab CI/CD"">Gitlab CI/CD</a>, etc.). By using it from Day 1, you instantly gain a single source of truth for requirements, progress, and historical data, which is essential for effective sprint planning, stakeholder communication, and post-release analysis.</p>"
Mermaid,adopt,Tools,TRUE,"<p>Mermaid is a powerful JavaScript-based diagramming tool that allows you to generate diagrams and charts from simple, Markdown-inspired text definitions <strong>(Diagrams as Code)</strong>. Instead of using a graphical editor, you define the structure of a diagram—such as a flowchart, sequence diagram, class diagram, or Gantt chart—using a readable text syntax. The library then renders this text into a visual, vector-based diagram (typically SVG) that can be easily displayed in web browsers or documentation.</p> <p>Mermaid solves a fundamental problem: <strong>Documentation Rot</strong>. Traditional image-based diagrams quickly become outdated as a project evolves and are tedious to update and impossible to version control effectively. By adopting Mermaid, diagrams become <strong>text-based assets</strong> that are stored alongside the source code. This aligns with the &quot;documentation as code&quot; principle, allowing diagrams to be <strong>version-controlled</strong>, reviewed via Pull Requests, and automatically rendered in tools like GitHub, GitLab, and most wiki/documentation platforms. This dramatically reduces the friction and cost of keeping technical diagrams accurate, ensuring that documentation remains a reliable and valuable resource from a project&#39;s inception.</p>"
Postman,adopt,Tools,TRUE,"<p><a href=""https://www.postman.com"">Postman</a> is an established API development and testing platform that has consistently proven its value across ASEE teams and projects. It streamlines the design, documentation, and validation of APIs through an intuitive interface for crafting requests, managing environments, and automating test scripts. Its strong support for REST, GraphQL, and SOAP APIs, along with built-in mocking, monitoring, and documentation capabilities, makes it a comprehensive solution for end-to-end API lifecycle management.</p> <p>We place Postman in the <strong>Adopt</strong> ring based on <strong>positive, widespread experience across multiple teams</strong>. It has demonstrated high usability, reliability, and effective collaboration through shared collections and workspaces. Integration with CI/CD pipelines enables automated API testing within DevOps workflows, reinforcing quality and consistency.</p> <p>For our group, Postman remains the standard tool for API design and testing. Its maturity and adoption rate make it the pragmatic choice for cross-team interoperability.</p>"
Trivy,adopt,Tools,TRUE,"<p><a href=""https://aquasecurity.github.io/trivy/"">Trivy</a> is a comprehensive, open-source security scanner for containers, Kubernetes, infrastructure-as-code, and more. It has emerged as the <strong>de-facto standard for scanning container images</strong>, offering fast and reliable vulnerability detection with minimal configuration overhead. Trivy supports multiple sources (OS packages, language-specific dependencies, config files) and integrates easily into CI/CD workflows and developer tooling.</p> <p>Beyond vulnerability scanning, Trivy also supports <strong>Software Bill of Materials (SBOM) generation</strong>, helping teams meet compliance, transparency, and traceability requirements for modern software supply chains. Its native support for SBOM formats such as SPDX and CycloneDX enables further tooling interoperability and regulatory alignment.</p> <p>We place Trivy in the <strong>Adopt</strong> ring due to its high usability, active development, and proven effectiveness in real-world use. Its broad protocol support, reliable results, and seamless DevSecOps integration make it a pragmatic choice for container security at scale. Trivy should be part of the standard toolkit for secure software delivery.</p>"
AI coding assistants,trial,Tools,TRUE,"<p>AI coding assistants such as <a href=""https://github.com/features/copilot"">GitHub Copilot</a>, <a href=""https://cursor.sh/"">Cursor</a>, <a href=""https://claude.ai/"">Claude Code</a>, and others are reshaping how developers write, understand, and maintain code. These tools go beyond simple autocomplete; they serve as conversational partners that can generate functions, suggest refactorings, write tests, explain code, or summarize large repositories. Many teams in our group report positive experience using AI assistants to reduce boilerplate, speed up debugging, and improve test coverage. Teams should use first opportunity to introduce use of AI coding assistants into their workflow.</p> <p>Integrated within IDEs or terminals, they support pair-programming workflows and accelerate onboarding by helping engineers query complex legacy systems conversationally. However, productivity gains come with governance and risk management requirements—avoiding sensitive data exposure, IP contamination, or over-reliance on unreviewed output. Other notable tools include <a href=""https://www.jetbrains.com/ai/"">JetBrains AI Assistant</a>, <a href=""https://aws.amazon.com/codewhisperer/"">Amazon CodeWhisperer</a>, <a href=""https://www.tabnine.com/"">Tabnine</a>, and <a href=""https://codeium.com/"">Codeium</a>.</p>"
Helmfile,trial,Tools,TRUE,"<p><a href=""https://helmfile.readthedocs.io/"">Helmfile</a> defines a declarative YAML specification to deploy and manage multiple Helm charts consistently across environments. It is one of the popular ways to orchestrate multiple Helm charts when deploying cloud-native applications, providing a more structured approach to managing complex deployments. Helmfile simplifies management of multiple Helm charts across different environments, enables better version control and reproducibility of deployments, and reduces complexity when managing microservices architectures.</p> <p>The tool integrates well with GitOps workflows such as ArgoCD and Flux for automated deployments, making it ideal for complex applications with multiple services requiring environment-specific configuration management. However, for simple projects with only one Helm chart or basic deployments, Helmfile adds unnecessary complexity and standard Helm commands are sufficient. Teams should be aware that Helmfile introduces an additional layer of abstraction over Helm, requiring understanding of both concepts, and debugging can be more complex due to this additional layer.</p>"
Shared credentials management,trial,Tools,TRUE,"<p>Centralised credentials and secret management (HashiCorp Vault, cloud secrets managers, external-secrets patterns) provide secure storage, access control, rotation and auditability for secrets used by applications and platform components. They decouple secret lifecycle from application code and improve compliance posture.</p> <p>Why now</p> <ul> <li>Increasing security and compliance requirements, plus multi-cloud and Kubernetes adoption, make centralised secret management an operational necessity for production systems.</li> </ul> <p>When to use</p> <ul> <li>Use a shared, centrally managed secrets solution for organisation-wide secrets, cross-team reuse, automated rotation, and when you need fine-grained access controls and audit trails. For Kubernetes workloads, prefer integrating with external secret operators rather than embedding long-lived secrets in manifests.</li> </ul> <p>Caveats / Considerations</p> <ul> <li>Operational complexity: provisioning, HA, backups, and secret leasing require careful planning.</li> <li>Access controls and least-privilege are essential — avoid over-broad roles and service accounts.</li> <li>Watch out for secret sprawl and high-cardinality telemetry when auditing secret access. Ensure secure injection patterns (sidecar, CSI driver) and consider short-lived credentials where possible.</li> </ul> <p>References: <a href=""https://www.vaultproject.io/"">https://www.vaultproject.io/</a> — <a href=""https://kubernetes.io/docs/concepts/configuration/secret/"">https://kubernetes.io/docs/concepts/configuration/secret/</a> — External Secrets operators (external-secrets.io)</p>"
SonarQube,trial,Tools,TRUE,"<p><a href=""https://www.sonarsource.com/products/sonarqube/"">SonarQube</a> is a code quality and security analysis platform that automatically scans source code for bugs, vulnerabilities, code smells, and security hotspots. Its comprehensive detection of both quality and security issues makes it attractive for maintaining high code standards and reducing security risks. SonarQube supports over 30 programming languages and integrates seamlessly with CI/CD pipelines.</p> <p>ASEE hosts the <strong>Enterprise Edition on ASEE Cloud</strong>, which is the <strong>preferred usage model</strong> providing advanced features like branch analysis, security reports, and portfolio management. <strong>Community Edition</strong> (open-source) and <strong>SonarCloud</strong> (SaaS) are available alternatives. Already integrated into the <strong>Secure Development Framework</strong>, we&#39;re placing SonarQube in <strong>trial</strong> as we evaluate its effectiveness in identifying vulnerabilities and enforcing quality standards across projects, particularly for <a href=""Spring Boot"">Spring Boot</a> and other Java applications. For teams prioritizing security and code quality, SonarQube provides automated, consistent enforcement that manual reviews alone cannot achieve.</p>"
Test management tools,trial,Tools,TRUE,"<p>Test management tools such as <a href=""https://www.testrail.com/"">TestRail</a>, <a href=""https://www.zephyr.com/"">Zephyr</a>, <a href=""https://www.tricentis.com/qtest/"">qTest</a>, and <a href=""https://azure.microsoft.com/en-us/services/devops/test-plans/"">Azure Test Plans</a> help organize, track, and manage the testing process throughout the software development lifecycle. These platforms provide centralized hubs for test planning, execution, reporting, and collaboration among team members, improving test organization and traceability across complex projects.</p> <p>Most tools integrate seamlessly with Jira, GitHub, Azure DevOps, CI/CD pipelines, and automation frameworks like Selenium or Playwright, making them valuable for projects with multiple testers and complex test scenarios. They offer comprehensive metrics, traceability matrices, and customizable dashboards that help maintain test coverage and quality metrics. However, teams should consider the additional tooling overhead, learning curve, and cost implications for enterprise features. Some tools are already being used successfully within the group, demonstrating their value for improving collaboration between testers and developers.</p>"
ZAP (OWASP ZAP),trial,Tools,TRUE,"<p><a href=""https://www.zaproxy.org/"">ZAP (Zed Attack Proxy)</a> is a free, open source tool for <a href=""https://owasp.org/www-project-devsecops-guideline/latest/02b-Dynamic-Application-Security-Testing"">Dynamic Application Security Testing (DAST)</a> and the de-facto standard in this domain. Part of the <a href=""https://owasp.org/"">OWASP ecosystem</a>, ZAP is one of the most widely used web application security scanners worldwide. It can find vulnerabilities in web applications and APIs by automatically crawling endpoints and scanning for issues like injection vulnerabilities, security misconfigurations, and other common security flaws.</p> <p>ZAP offers both passive scanning (observing traffic for vulnerabilities) and active scanning (automated attacks on targets), backed by a rich set of plugins and an automation-friendly design. Teams in our organization use ZAP for API security testing and have scripted its REST API for integration into CI/CD pipelines. This allows developers to include ZAP scans in build or deployment workflows, performing continuous security checks without significant manual effort. So far, we have not encountered any notable limitations in ZAP’s capabilities, which speaks to its maturity and reliability as a go-to DAST solution.</p> <p>We place OWASP ZAP in the Trial ring based on our positive internal experience and its proven value in improving application security. As a next step, we recommend broader adoption by integrating ZAP into the software development lifecycle (SDLC) – for example, running automated ZAP scans as part of regular development and QA processes. By embedding ZAP into pipelines, teams can catch vulnerabilities early and reinforce a security-first mindset throughout development.</p>"
Agentic workflow tools,assess,Tools,TRUE,"<p>Agentic workflow tools orchestrate AI systems that autonomously plan, execute, and iterate on multi-step tasks via reasoning, tool use, and memory. Modern stacks include graph/state orchestrators like <a href=""https://langchain-ai.github.io/langgraph/"">LangGraph</a> and <a href=""https://www.llamaindex.ai/workflows"">LlamaIndex Workflows</a>, multi-agent frameworks like <a href=""https://www.crewai.com/"">CrewAI</a> and <a href=""https://openai.github.io/openai-agents-python/"">OpenAI Agents SDK</a> (evolved from <a href=""https://github.com/openai/swarm"">Swarm</a>), and enterprise/low-code options such as <a href=""https://n8n.io/ai-agents/"">n8n AI Agents</a> and platform kits like <a href=""https://openai.com/agent-platform/"">OpenAI AgentKit</a>. These tools power patterns like repo-wide refactoring, self-correcting test generation, incident triage, knowledge-base synthesis, and long-running build troubleshooting. Risks remain: flaky autonomy, complex debugging/observability, permissions &amp; data-safety, and runaway costs from deep call-chains. <strong>Assess</strong> for bounded internal use (developer tooling, data/ops pipelines) with human-in-the-loop controls, clear rollback paths, spend guards, and production telemetry before placing on critical paths.</p>"
Atlas,assess,Tools,TRUE,"<p><a href=""https://atlasgo.io/"">Atlas</a> is a modern declarative database schema management framework designed to bring <strong>infrastructure-as-code principles</strong> to relational database evolution.  It differs from traditional migration tools like <a href=""https://flywaydb.org/"">Flyway</a> or <a href=""https://www.liquibase.org/"">Liquibase</a> by maintaining a single canonical schema state and automatically generating migration diffs.  This eliminates manual script drift and simplifies CI/CD validation. Atlas can integrate with GitOps pipelines, enforcing schema consistency across environments and validating schema drift before deployment.  Its declarative DSL also supports modular schema composition, which helps teams manage multiple services or tenants while ensuring controlled expand/contract migrations.  Digital Platform is assessing Atlas as a unifying tool for schema governance across multiple data stores that evolve independently but must remain compliant with shared standards and auditability requirements.</p>"
GitHub Actions,assess,Tools,TRUE,"<p><a href=""https://docs.github.com/en/actions"">GitHub Actions</a> is a continuous integration and continuous delivery platform that allows you to automate your build, test, and deployment pipeline directly within GitHub repositories. It provides a rich ecosystem of pre-built actions and workflows available in the <a href=""https://github.com/marketplace?type=actions"">GitHub Marketplace</a>, aligning with our strategic direction for automation and DevOps practices. The platform offers seamless integration with GitHub&#39;s version control and project management features, supporting reusable workflows across repositories and integration with third-party secrets managers.</p> <p>GitHub Actions is ideal for projects hosted on GitHub that require CI/CD automation and want to leverage the extensive ecosystem of pre-built actions. However, teams should be aware of potential vendor lock-in, as migration to another CI/CD platform like GitLab CI or Jenkins may require full workflow rewrites due to GitHub-specific syntax and features. Additionally, complex workflow configurations have a learning curve, and resource limitations on the free tier may affect larger projects. Teams not already using GitHub for repository hosting should evaluate whether this requirement aligns with their infrastructure strategy.</p>"
Grafana observability stack for IM and APM,assess,Tools,TRUE,"<p>Modern observability stacks have become essential for understanding system behavior in distributed architectures, providing unified visibility across metrics, logs, and traces — the three pillars of observability. The <a href=""https://grafana.com/"">Grafana</a> observability stack, combining Grafana <a href=""https://grafana.com/oss/loki/"">Loki</a> for logs, <a href=""https://grafana.com/oss/tempo/"">Tempo</a> for traces, and <a href=""https://grafana.com/oss/mimir/"">Mimir</a> or <a href=""https://prometheus.io/"">Prometheus</a> for metrics, represents a compelling open-source alternative for infrastructure monitoring (IM) and application performance monitoring (APM). We&#39;re seeing increased interest in this unified approach as teams seek to consolidate their observability tooling while maintaining flexibility and cost control. The stack&#39;s strength lies in its integration—correlated metrics, logs, and traces within a single interface significantly improve troubleshooting efficiency compared to disparate tools. Grafana&#39;s rich visualization capabilities and extensive plugin ecosystem enable teams to build comprehensive dashboards tailored to their specific needs. </p> <p>We&#39;re placing the Grafana stack in <strong>assess</strong> ring as it shows strong potential as candidate for standardized observability stack within our group which would simplify operations for our customers and cloud. Teams should have this in mind when evaluating alternatives like the <a href=""https://www.elastic.co/observability"">Elastic Observability</a>, or commercial offerings like <a href=""https://www.dynatrace.com/"">Dynatrace</a> and <a href=""https://www.ibm.com/products/instana"">IBM Instana</a>. Teams should pilot the Grafana stack on non-critical environments first, evaluate the total cost of ownership including operational effort, and ensure they have the expertise to tune performance. As our group prioritizes vendor independence and cost optimization over turnkey solutions, the Grafana observability stack offers a powerful and flexible foundation. The main considerations are the operational overhead of running and maintaining multiple components, query language learning curves (<a href=""https://grafana.com/docs/loki/latest/query/"">LogQL</a>, <a href=""https://prometheus.io/docs/prometheus/latest/querying/basics/"">PromQL</a>), and scaling challenges at very high data volumes.</p>"
K6,assess,Tools,TRUE,"<p><a href=""https://k6.io/"">K6</a> represents a modern approach to performance testing, built around a <strong>developer-centric philosophy</strong> and <strong>JavaScript-based scripting</strong>. The tool treats performance testing as code, enabling version control of test scripts, seamless CI/CD integration, and maintenance alongside application code. Its cloud-native architecture includes built-in support for modern protocols like HTTP/2, WebSockets, and gRPC, making it particularly well-suited for testing microservices and API endpoints.</p> <p>K6 provides <strong>real-time metrics</strong> and <strong>threshold-based assertions</strong> that deliver immediate feedback during test execution. The tool&#39;s <strong>extensible architecture</strong> supports custom metrics and integrates with monitoring tools like <a href=""https://grafana.com/"">Grafana</a> and <a href=""https://www.influxdata.com/"">InfluxDB</a>. Its <strong>lightweight footprint</strong> and <strong>container-friendly design</strong> make it ideal for distributed testing scenarios and cloud-native environments.</p> <p>The tool&#39;s popularity stems from its ability to address the limitations of traditional performance testing tools like <a href=""https://jmeter.apache.org/"">JMeter</a>, particularly in environments where teams prefer code-based approaches over GUI-driven test creation. While K6 shows significant promise and addresses real pain points in performance testing, it&#39;s still new compared to established tools, and its JavaScript dependency and learning curve may present challenges for teams with limited frontend expertise. Organizations should assess K6&#39;s fit for their specific testing needs and team capabilities before committing to adoption.</p>"
Playwright,assess,Tools,TRUE,"<p>Playwright is a modern end-to-end (E2E) testing framework for web apps that automates real browsers—Chromium, Firefox, and WebKit—with one API. It ships with a test runner, auto-waiting actions, resilient locators, network mocking, screenshots/videos, and a trace viewer. Teams use it to validate critical user flows, catch regressions across browsers, and debug flaky UI behavior with rich artifacts.</p> <p>It shines in CI and local dev alike. Parallel execution and isolated browser contexts keep tests fast and independent; fixtures manage auth and setup; retries + expect-polling reduce flake. Supports API testing, file uploads/downloads, geolocation, permissions, time control, and mobile emulation—plus component testing for frameworks like React/Vue.</p> <p>UI tests are inherently slower and costlier to maintain than unit/integration tests. It’s not a load/performance tool, and real-device coverage typically requires emulation or third-party device clouds. Too many E2E tests will slow pipelines and increase noise.</p> <p>Use Playwright to cover your critical paths (auth, checkout, dashboards) across browsers. Add Playwright for confidence on high-value flows. Adopt best practices: stable locators (data-testid), per-test isolation, deterministic test data, sensible timeouts, retries only for known flake, and artifact collection (screenshots, videos, traces) on failure.</p>"
Azure DevOps,hold,Tools,TRUE,"<p><a href=""https://azure.microsoft.com/en-us/products/devops"">Azure DevOps</a> is a comprehensive DevOps platform providing integrated services for version control, work tracking, continuous integration, and deployment. Its suite includes Azure Repos for Git version control, Azure Boards for agile planning and work tracking, Azure Pipelines for CI/CD, and Azure Artifacts for package management, all deeply integrated with the Microsoft cloud ecosystem.</p> <p>While Azure DevOps offers mature capabilities and enterprise-grade security, our experience has revealed persistent challenges. The platform suffers from limited customization, complex licensing, and sub-optimal integration with non-Microsoft tools. Vendor lock-in creates dependency risks, and its user interface and workflows often feel dated compared to more modern, open alternatives like <a href=""https://github.com/"">GitHub</a> and <a href=""https://www.atlassian.com/software/jira"">Jira</a>. Although Microsoft has modernized parts of the product—introducing YAML-based pipelines and improved GitHub integration—these changes have not fully addressed usability and ecosystem concerns.</p> <p>Teams heavily invested in Azure DevOps may continue using it for stable workloads but should plan gradual migration toward more flexible, ecosystem-agnostic solutions that enhance developer experience and agility.</p>"
Docker Desktop,hold,Tools,TRUE,"<p>Docker Desktop once served as the de facto way to manage local container environments on Windows, macOS, and Linux. It bundled Docker Engine, CLI, BuildKit, Compose, and an intuitive GUI that simplified networking, file sharing, and local Kubernetes. However, with the shift to a <strong>commercial licensing model for business use</strong>, it no longer fits most enterprise setups where multiple developers or teams use shared tooling.  </p> <p>The tool remains valuable for individual developers and small proof-of-concept work but introduces cost, compliance, and parity issues in regulated or large-scale environments. Alternatives such as <a href=""https://rancherdesktop.io/"">Rancher Desktop</a>, <a href=""https://podman.io/"">Podman Desktop</a>, <a href=""https://minikube.sigs.k8s.io/"">Minikube</a>, <a href=""https://kind.sigs.k8s.io/"">kind</a>, or <a href=""https://github.com/abiosoft/colima"">Colima</a> offer free, open options that align better with production parity and cost control.  </p> <p>Teams should <strong>hold</strong> Docker Desktop for new adoption and favor standardized setups using Docker Engine or Podman directly, ensuring licensing clarity and consistent developer experience across environments.</p>"
Standalone CodeQL,hold,Tools,TRUE,"<p><a href=""https://codeql.github.com/docs/"">CodeQL</a> is GitHub&#39;s semantic code analysis engine that allows you to query code as if it were data, providing powerful static analysis capabilities for identifying security vulnerabilities that traditional linters might miss. While <a href=""https://docs.github.com/en/enterprise-cloud@latest/admin/advanced-security"">GitHub Advanced Security</a> provides integrated CodeQL analysis within GitHub repositories with automatic scanning, standalone CodeQL requires manual setup and is restricted by licensing—standalone use is available only within GitHub Advanced Security subscriptions.</p> <p>The key challenge with standalone CodeQL is the licensing restriction, which requires a GitHub Advanced Security license. Additionally, it comes with a learning curve for writing custom queries and can be resource-intensive for large codebases. For teams seeking alternatives without licensing restrictions, <a href=""https://semgrep.dev/"">Semgrep</a> offers an open-source static analysis tool with community rules, while <a href=""https://www.sonarsource.com/products/sonarqube/"">SonarQube Community Edition</a> provides open-source code quality and security analysis. Given these licensing constraints and available alternatives, teams should carefully evaluate whether standalone CodeQL fits their needs and budget.</p>"